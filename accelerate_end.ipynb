{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d16f578a02824d6dbeacfeeebbaac306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ef68ab897c214410acaffabb565673df",
              "IPY_MODEL_b57dfd95bf9c4ab6abd687c372a3b9a4",
              "IPY_MODEL_f764df4830b04c5dabca1780bd3fc165"
            ],
            "layout": "IPY_MODEL_fbc44e308cd8493ba9a116f11254efa5"
          }
        },
        "ef68ab897c214410acaffabb565673df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16ebe26f03d6412d952657aa4fd3c60a",
            "placeholder": "​",
            "style": "IPY_MODEL_2d31b9d69a814413bb05b131e92c28b7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b57dfd95bf9c4ab6abd687c372a3b9a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c3459fa6ac94207bd4e589f32b8e8c5",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6c2153c6e454509a5ac98c5696a03b4",
            "value": 4
          }
        },
        "f764df4830b04c5dabca1780bd3fc165": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_521972678d9d41b38bb29906f10e0d40",
            "placeholder": "​",
            "style": "IPY_MODEL_5b413a0f742c4ca186b0f85a6169774f",
            "value": " 4/4 [00:06&lt;00:00,  1.51s/it]"
          }
        },
        "fbc44e308cd8493ba9a116f11254efa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16ebe26f03d6412d952657aa4fd3c60a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d31b9d69a814413bb05b131e92c28b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c3459fa6ac94207bd4e589f32b8e8c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6c2153c6e454509a5ac98c5696a03b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "521972678d9d41b38bb29906f10e0d40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b413a0f742c4ca186b0f85a6169774f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4092892cc7694758baa2435b95b36c39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b086478735de4cc9a9f45ff39d5a7d1b",
              "IPY_MODEL_69fe3b879ab64be4828da92317a0291f",
              "IPY_MODEL_d2c3b2bbefd64fb68f5e8428396692b2"
            ],
            "layout": "IPY_MODEL_e6d9cb2b2ca84b7ab95be03e85fec670"
          }
        },
        "b086478735de4cc9a9f45ff39d5a7d1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_483ce3f2de80446fafbe59c580e25154",
            "placeholder": "​",
            "style": "IPY_MODEL_f550a8058b3b4f8abe672c3c082e1f2d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "69fe3b879ab64be4828da92317a0291f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a770a89ecbd54508aba93c715403b40a",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_205d60208c464562bbb36a32ee0d34df",
            "value": 4
          }
        },
        "d2c3b2bbefd64fb68f5e8428396692b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6e6adcb87fe45548081a2bca3528c77",
            "placeholder": "​",
            "style": "IPY_MODEL_c52eedc0550e4b0787c27bfc138f6527",
            "value": " 4/4 [00:01&lt;00:00,  1.62it/s]"
          }
        },
        "e6d9cb2b2ca84b7ab95be03e85fec670": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "483ce3f2de80446fafbe59c580e25154": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f550a8058b3b4f8abe672c3c082e1f2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a770a89ecbd54508aba93c715403b40a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "205d60208c464562bbb36a32ee0d34df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e6e6adcb87fe45548081a2bca3528c77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c52eedc0550e4b0787c27bfc138f6527": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55470e64e7c74730bb57d4935c64505f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_16077809eb994d04be966a0c7febdb6f",
              "IPY_MODEL_65b27ec1d6ca43c1a4ff374904e47f6b",
              "IPY_MODEL_96790876973042ecac7964741d83c650"
            ],
            "layout": "IPY_MODEL_204351b7661c46c3ae27c4762fea414b"
          }
        },
        "16077809eb994d04be966a0c7febdb6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfa48c7683a14e8098489c65da545cdc",
            "placeholder": "​",
            "style": "IPY_MODEL_6495201328444ac8b3230dec2b181c44",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "65b27ec1d6ca43c1a4ff374904e47f6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc674e3934654feca47e1358919dd157",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7fa03f1e015a4c03880a4cb6351b314b",
            "value": 4
          }
        },
        "96790876973042ecac7964741d83c650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94dd2bf23c544698b59006db30a8c123",
            "placeholder": "​",
            "style": "IPY_MODEL_8591260931104b5788713b6fefdfbe4a",
            "value": " 4/4 [00:00&lt;00:00,  3.24it/s]"
          }
        },
        "204351b7661c46c3ae27c4762fea414b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfa48c7683a14e8098489c65da545cdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6495201328444ac8b3230dec2b181c44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc674e3934654feca47e1358919dd157": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fa03f1e015a4c03880a4cb6351b314b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "94dd2bf23c544698b59006db30a8c123": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8591260931104b5788713b6fefdfbe4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS0hkzYRMnd5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Vd37tSOMt_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RpJZszrMMuCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4SGREYypMuF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct"
      ],
      "metadata": {
        "id": "KmaODlzvtADc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/install/pip"
      ],
      "metadata": {
        "id": "Pd4YCFkNs6Sr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzBee5nOMuIe",
        "outputId": "325100e3-3313-45b6-efac-4d1ac5f97541"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-n2qum5cf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-n2qum5cf\n",
            "  Resolved https://github.com/huggingface/transformers to commit fa3f2db5c7405a742fcb8f686d3754f70db00977\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (2.32.3)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers==4.46.0.dev0)\n",
            "  Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.0.dev0) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.0.dev0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46.0.dev0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46.0.dev0) (2024.8.30)\n",
            "Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.46.0.dev0-py3-none-any.whl size=9972254 sha256=3e0aa2a969440224d79218a36075dd62b722983fe1f54fefb64d293054e725c9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cjs7q0wk/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "Successfully installed tokenizers-0.20.1 transformers-4.46.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9M4fWTk_Opxh",
        "outputId": "2456700c-71ac-48bb-f018-9a70cbf0ff8e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.24.7)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "device = accelerator.device\n",
        "model, optimizer, data = accelerator.prepare(model, optimizer, data)\n",
        "accelerator.backward(loss)"
      ],
      "metadata": {
        "id": "dYgXRHt3OAfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcbA7uI3O1d4",
        "outputId": "480621fa-c8db-49be-bf75-57c3ac5e740c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from accelerate import Accelerator\n",
        "import gc\n",
        "gc.collect()\n",
        "accelerator = Accelerator()\n",
        "\n",
        "device = accelerator.device\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "#model, optimizer, data = accelerator.prepare(model_name, optimizer, data)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "import gc\n",
        "gc.collect()\n",
        "prompt = \"Give me a short introduction to large language model.\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "import gc\n",
        "gc.collect()\n",
        "accelerator.backward(loss)\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478,
          "referenced_widgets": [
            "d16f578a02824d6dbeacfeeebbaac306",
            "ef68ab897c214410acaffabb565673df",
            "b57dfd95bf9c4ab6abd687c372a3b9a4",
            "f764df4830b04c5dabca1780bd3fc165",
            "fbc44e308cd8493ba9a116f11254efa5",
            "16ebe26f03d6412d952657aa4fd3c60a",
            "2d31b9d69a814413bb05b131e92c28b7",
            "5c3459fa6ac94207bd4e589f32b8e8c5",
            "a6c2153c6e454509a5ac98c5696a03b4",
            "521972678d9d41b38bb29906f10e0d40",
            "5b413a0f742c4ca186b0f85a6169774f"
          ]
        },
        "id": "l_IX5gH3MuL2",
        "outputId": "3c5cf963-b326-4938-f597-5afe7913ab56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d16f578a02824d6dbeacfeeebbaac306"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ff58179da0b7>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m generated_ids = model.generate(\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2206\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2207\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2208\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3171\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3172\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3174\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1189\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 )\n\u001b[1;32m    919\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    921\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_state)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mpre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m             ):\n\u001b[1;32m    335\u001b[0m                 \u001b[0mfp16_statistics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"weight\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weight\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SCB\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/offload.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{self.prefix}{key}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/offload.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0msafe_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"safetensors_file\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weight_name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# if failed to get_tensor on the device, such as bf16 on mps, try to load it on CPU first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW\n",
        "from accelerate import Accelerator\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "accelerator = Accelerator()\n",
        "\n",
        "device = accelerator.device\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Create an optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)  # You can adjust the learning rate\n",
        "\n",
        "# Create a sample dataset (replace with your actual data)\n",
        "data = torch.randint(0, tokenizer.vocab_size, (16, 128))\n",
        "\n",
        "# Prepare the model, optimizer, and data with the accelerator\n",
        "model, optimizer, data = accelerator.prepare(model, optimizer, data)\n",
        "model = torch.nn.Transformer()\n",
        "# Rest of your code...\n",
        "prompt = \"Give me a short introduction to large language model.\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "import gc\n",
        "gc.collect()\n",
        "# Define your loss function and calculate the loss (replace with your actual loss calculation)\n",
        "# Example:\n",
        "# loss = some_loss_function(model_outputs, targets)\n",
        "\n",
        "# Backpropagate the loss using the accelerator\n",
        "# accelerator.backward(loss)\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478,
          "referenced_widgets": [
            "4092892cc7694758baa2435b95b36c39",
            "b086478735de4cc9a9f45ff39d5a7d1b",
            "69fe3b879ab64be4828da92317a0291f",
            "d2c3b2bbefd64fb68f5e8428396692b2",
            "e6d9cb2b2ca84b7ab95be03e85fec670",
            "483ce3f2de80446fafbe59c580e25154",
            "f550a8058b3b4f8abe672c3c082e1f2d",
            "a770a89ecbd54508aba93c715403b40a",
            "205d60208c464562bbb36a32ee0d34df",
            "e6e6adcb87fe45548081a2bca3528c77",
            "c52eedc0550e4b0787c27bfc138f6527"
          ]
        },
        "id": "CmsgPvhmPlEB",
        "outputId": "fa81056a-0bfd-42bf-d376-55710b89a34b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4092892cc7694758baa2435b95b36c39"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the disk and cpu.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-5e4a8f981265>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m generated_ids = model.generate(\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2206\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2207\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2208\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3171\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3172\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3174\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1189\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 )\n\u001b[1;32m    919\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    921\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_state)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate config -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lqa27SD1Srzz",
        "outputId": "852f053c-f2ae-46ad-bdea-3d063a07c608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: accelerate <command> [<args>] config [-h] [--config_file CONFIG_FILE] {default,update} ...\n",
            "\n",
            "Launches a series of prompts to create and save a `default_config.yaml` configuration file for\n",
            "your training system. Should always be ran first on your machine\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --config_file CONFIG_FILE, --config-file CONFIG_FILE\n",
            "                        The path to use to store the config file. Will default to a file named\n",
            "                        default_config.yaml in the cache location, which is the content of the\n",
            "                        environment `HF_HOME` suffixed with 'accelerate', or if you don't have\n",
            "                        such an environment variable, your cache directory ('~/.cache' or the\n",
            "                        content of `XDG_CACHE_HOME`) suffixed with 'huggingface'.\n",
            "\n",
            "subcommands:\n",
            "  {default,update}\n",
            "    default             Create a default config file for Accelerate with only a few flags set.\n",
            "    update              Update an existing config file with the latest defaults while maintaining\n",
            "                        the old configuration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate config /content/single_gpu.yaml"
      ],
      "metadata": {
        "id": "9pudJY8GR0Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch --config_file /content/single_gpu.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjnB51LYR1J5",
        "outputId": "494aef9f-97c7-41f8-8be6-ec02666368f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: accelerate <command> [<args>] launch [-h] [--config_file CONFIG_FILE] [--quiet] [--cpu]\n",
            "                                            [--multi_gpu] [--tpu] [--ipex]\n",
            "                                            [--mixed_precision {no,fp16,bf16,fp8}]\n",
            "                                            [--num_processes NUM_PROCESSES]\n",
            "                                            [--num_machines NUM_MACHINES]\n",
            "                                            [--num_cpu_threads_per_process NUM_CPU_THREADS_PER_PROCESS]\n",
            "                                            [--enable_cpu_affinity]\n",
            "                                            [--dynamo_backend {no,eager,aot_eager,inductor,aot_ts_nvfuser,nvprims_nvfuser,cudagraphs,ofi,fx2trt,onnxrt,tensorrt,aot_torchxla_trace_once,torhchxla_trace_once,ipex,tvm}]\n",
            "                                            [--dynamo_mode {default,reduce-overhead,max-autotune}]\n",
            "                                            [--dynamo_use_fullgraph] [--dynamo_use_dynamic]\n",
            "                                            [--use_deepspeed] [--use_fsdp] [--use_megatron_lm]\n",
            "                                            [--use_xpu] [--gpu_ids GPU_IDS] [--same_network]\n",
            "                                            [--machine_rank MACHINE_RANK]\n",
            "                                            [--main_process_ip MAIN_PROCESS_IP]\n",
            "                                            [--main_process_port MAIN_PROCESS_PORT] [-t TEE]\n",
            "                                            [--log_dir LOG_DIR] [--role ROLE]\n",
            "                                            [--rdzv_backend RDZV_BACKEND] [--rdzv_conf RDZV_CONF]\n",
            "                                            [--max_restarts MAX_RESTARTS]\n",
            "                                            [--monitor_interval MONITOR_INTERVAL] [-m]\n",
            "                                            [--no_python] [--tpu_cluster] [--no_tpu_cluster]\n",
            "                                            [--tpu_use_sudo] [--vm VM] [--env ENV]\n",
            "                                            [--main_training_function MAIN_TRAINING_FUNCTION]\n",
            "                                            [--downcast_bf16]\n",
            "                                            [--deepspeed_config_file DEEPSPEED_CONFIG_FILE]\n",
            "                                            [--zero_stage ZERO_STAGE]\n",
            "                                            [--offload_optimizer_device OFFLOAD_OPTIMIZER_DEVICE]\n",
            "                                            [--offload_param_device OFFLOAD_PARAM_DEVICE]\n",
            "                                            [--offload_optimizer_nvme_path OFFLOAD_OPTIMIZER_NVME_PATH]\n",
            "                                            [--offload_param_nvme_path OFFLOAD_PARAM_NVME_PATH]\n",
            "                                            [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
            "                                            [--gradient_clipping GRADIENT_CLIPPING]\n",
            "                                            [--zero3_init_flag ZERO3_INIT_FLAG]\n",
            "                                            [--zero3_save_16bit_model ZERO3_SAVE_16BIT_MODEL]\n",
            "                                            [--deepspeed_hostfile DEEPSPEED_HOSTFILE]\n",
            "                                            [--deepspeed_exclusion_filter DEEPSPEED_EXCLUSION_FILTER]\n",
            "                                            [--deepspeed_inclusion_filter DEEPSPEED_INCLUSION_FILTER]\n",
            "                                            [--deepspeed_multinode_launcher DEEPSPEED_MULTINODE_LAUNCHER]\n",
            "                                            [--deepspeed_moe_layer_cls_names DEEPSPEED_MOE_LAYER_CLS_NAMES]\n",
            "                                            [--fsdp_offload_params FSDP_OFFLOAD_PARAMS]\n",
            "                                            [--fsdp_min_num_params FSDP_MIN_NUM_PARAMS]\n",
            "                                            [--fsdp_sharding_strategy FSDP_SHARDING_STRATEGY]\n",
            "                                            [--fsdp_auto_wrap_policy FSDP_AUTO_WRAP_POLICY]\n",
            "                                            [--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]\n",
            "                                            [--fsdp_backward_prefetch_policy FSDP_BACKWARD_PREFETCH_POLICY]\n",
            "                                            [--fsdp_backward_prefetch FSDP_BACKWARD_PREFETCH]\n",
            "                                            [--fsdp_state_dict_type FSDP_STATE_DICT_TYPE]\n",
            "                                            [--fsdp_forward_prefetch FSDP_FORWARD_PREFETCH]\n",
            "                                            [--fsdp_use_orig_params FSDP_USE_ORIG_PARAMS]\n",
            "                                            [--fsdp_cpu_ram_efficient_loading FSDP_CPU_RAM_EFFICIENT_LOADING]\n",
            "                                            [--fsdp_sync_module_states FSDP_SYNC_MODULE_STATES]\n",
            "                                            [--fsdp_activation_checkpointing FSDP_ACTIVATION_CHECKPOINTING]\n",
            "                                            [--megatron_lm_tp_degree MEGATRON_LM_TP_DEGREE]\n",
            "                                            [--megatron_lm_pp_degree MEGATRON_LM_PP_DEGREE]\n",
            "                                            [--megatron_lm_num_micro_batches MEGATRON_LM_NUM_MICRO_BATCHES]\n",
            "                                            [--megatron_lm_sequence_parallelism MEGATRON_LM_SEQUENCE_PARALLELISM]\n",
            "                                            [--megatron_lm_recompute_activations MEGATRON_LM_RECOMPUTE_ACTIVATIONS]\n",
            "                                            [--megatron_lm_use_distributed_optimizer MEGATRON_LM_USE_DISTRIBUTED_OPTIMIZER]\n",
            "                                            [--megatron_lm_gradient_clipping MEGATRON_LM_GRADIENT_CLIPPING]\n",
            "                                            [--fp8_backend {te,msamp}]\n",
            "                                            [--fp8_use_autocast_during_eval]\n",
            "                                            [--fp8_margin FP8_MARGIN]\n",
            "                                            [--fp8_interval FP8_INTERVAL]\n",
            "                                            [--fp8_format {E4M3,HYBRID}]\n",
            "                                            [--fp8_amax_history_len FP8_AMAX_HISTORY_LEN]\n",
            "                                            [--fp8_amax_compute_algo {max,most_recent}]\n",
            "                                            [--fp8_override_linear_precision FP8_OVERRIDE_LINEAR_PRECISION]\n",
            "                                            [--fp8_opt_level {O1,O2}]\n",
            "                                            [--aws_access_key_id AWS_ACCESS_KEY_ID]\n",
            "                                            [--aws_secret_access_key AWS_SECRET_ACCESS_KEY]\n",
            "                                            [--debug] [--mpirun_hostfile MPIRUN_HOSTFILE]\n",
            "                                            [--mpirun_ccl MPIRUN_CCL]\n",
            "                                            training_script ...\n",
            "accelerate <command> [<args>] launch: error: the following arguments are required: training_script, training_script_args\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DQQ4iOxlR1NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "prompt = \"Give me a short introduction to large language model.\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443,
          "referenced_widgets": [
            "55470e64e7c74730bb57d4935c64505f",
            "16077809eb994d04be966a0c7febdb6f",
            "65b27ec1d6ca43c1a4ff374904e47f6b",
            "96790876973042ecac7964741d83c650",
            "204351b7661c46c3ae27c4762fea414b",
            "cfa48c7683a14e8098489c65da545cdc",
            "6495201328444ac8b3230dec2b181c44",
            "dc674e3934654feca47e1358919dd157",
            "7fa03f1e015a4c03880a4cb6351b314b",
            "94dd2bf23c544698b59006db30a8c123",
            "8591260931104b5788713b6fefdfbe4a"
          ]
        },
        "id": "NLQQKVA3R1SS",
        "outputId": "9bb81029-fe6d-446b-8c23-12682068ed67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55470e64e7c74730bb57d4935c64505f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-14b2fed088fa>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m generated_ids = model.generate(\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2206\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2207\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2208\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3171\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3172\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3174\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1189\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 )\n\u001b[1;32m    919\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    921\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_state)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z-NgBH95Xj7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TXuZaQWTXkce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0vvMoHhuXkfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWO6KgY-Xkhy",
        "outputId": "9e32f1b3-de12-4fe0-f952-2a2fc22a9543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Copy-and-paste the text below in your GitHub issue\n",
            "\n",
            "- `Accelerate` version: 0.34.2\n",
            "- Platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "- `accelerate` bash location: /usr/local/bin/accelerate\n",
            "- Python version: 3.10.12\n",
            "- Numpy version: 1.26.4\n",
            "- PyTorch version (GPU?): 2.4.1+cu121 (True)\n",
            "- PyTorch XPU available: False\n",
            "- PyTorch NPU available: False\n",
            "- PyTorch MLU available: False\n",
            "- PyTorch MUSA available: False\n",
            "- System RAM: 12.67 GB\n",
            "- GPU type: Tesla T4\n",
            "- `Accelerate` default config:\n",
            "\tNot found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huggingface/accelerate.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcNLuM5NXkkT",
        "outputId": "e486bc9c-72e2-45f6-f091-127f8b2a1556"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'accelerate'...\n",
            "remote: Enumerating objects: 14731, done.\u001b[K\n",
            "remote: Counting objects: 100% (1318/1318), done.\u001b[K\n",
            "remote: Compressing objects: 100% (83/83), done.\u001b[K\n",
            "remote: Total 14731 (delta 1279), reused 1235 (delta 1234), pack-reused 13413 (from 1)\u001b[K\n",
            "Receiving objects: 100% (14731/14731), 4.80 MiB | 10.62 MiB/s, done.\n",
            "Resolving deltas: 100% (10269/10269), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/accelerate/examples"
      ],
      "metadata": {
        "id": "KoeiG8jhcGLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'accelerate>=0.27.0' 'torchpippy>=0.2.0'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qniL_6acTyH",
        "outputId": "46ecf4e0-6763-402f-dadb-d670a82b3485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate>=0.27.0 in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Collecting torchpippy>=0.2.0\n",
            "  Downloading torchpippy-0.2.0-py3-none-any.whl.metadata (672 bytes)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.0) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.0) (2.4.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.0) (0.24.7)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.0) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.27.0) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.27.0) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.27.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.27.0) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.27.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.27.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.27.0) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.27.0) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate>=0.27.0) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.27.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.27.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.27.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.27.0) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate>=0.27.0) (1.3.0)\n",
            "Downloading torchpippy-0.2.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchpippy\n",
            "Successfully installed torchpippy-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "33NZqcLgeJsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch --num_processes 1 /content/accelerate/examples/inference/distributed/phi2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kN-eCik7cX-m",
        "outputId": "e0ac633d-e4b1-4efc-c378-db5ec08d991c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "Loading checkpoint shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/accelerate/examples/inference/distributed/phi2.py\", line 27, in <module>\n",
            "    model = AutoModelForCausalLM.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n",
            "    return model_class.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 4191, in from_pretrained\n",
            "    ) = cls._load_pretrained_model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 4694, in _load_pretrained_model\n",
            "    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 991, in _load_state_dict_into_meta_model\n",
            "    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py\", line 416, in set_module_tensor_to_device\n",
            "    new_value = value.to(device)\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 11.06 MiB is free. Process 3961 has 12.29 GiB memory in use. Process 216431 has 2.45 GiB memory in use. Of the allocated memory 2.27 GiB is allocated by PyTorch, and 82.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 1174, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 769, in simple_launcher\n",
            "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
            "subprocess.CalledProcessError: Command '['/usr/bin/python3', '/content/accelerate/examples/inference/distributed/phi2.py']' returned non-zero exit status 1.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!accelerate launch --num_machines=1 --mixed_precision=fp16 --dynamo_backend=no /content/accelerate/examples/inference/distributed/phi2.py"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbnqOmBQfG5n",
        "outputId": "961f169b-dddd-4413-f3a8-574dbf98c06b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "Loading checkpoint shards:   0% 0/2 [00:01<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/accelerate/examples/inference/distributed/phi2.py\", line 27, in <module>\n",
            "    model = AutoModelForCausalLM.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n",
            "    return model_class.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 4191, in from_pretrained\n",
            "    ) = cls._load_pretrained_model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 4694, in _load_pretrained_model\n",
            "    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 991, in _load_state_dict_into_meta_model\n",
            "    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py\", line 416, in set_module_tensor_to_device\n",
            "    new_value = value.to(device)\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 11.06 MiB is free. Process 3961 has 12.29 GiB memory in use. Process 223002 has 2.45 GiB memory in use. Of the allocated memory 2.27 GiB is allocated by PyTorch, and 82.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 1174, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 769, in simple_launcher\n",
            "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
            "subprocess.CalledProcessError: Command '['/usr/bin/python3', '/content/accelerate/examples/inference/distributed/phi2.py']' returned non-zero exit status 1.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!accelerate launch --config_file default_config.yaml /content/accelerate/examples/inference/distributed/phi2.py"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcnUtbO9fHdm",
        "outputId": "5ea6d346-34dc-4088-f946-5ca5a294f1f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 1152, in launch_command\n",
            "    args, defaults, mp_from_config_flag = _validate_launch_command(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 986, in _validate_launch_command\n",
            "    defaults = load_config_from_file(args.config_file)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/config/config_args.py\", line 46, in load_config_from_file\n",
            "    raise FileNotFoundError(\n",
            "FileNotFoundError: The passed configuration file `default_config.yaml` does not exist. Please pass an existing file to `accelerate launch`, or use the default one created through `accelerate config` and run `accelerate launch` without the `--config_file` argument.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!accelerate launch --config_file default_config.yaml /content/accelerate/examples/inference/distributed/phi2.py"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ifUSmzZfJhu",
        "outputId": "3165d713-90ea-41f7-f61e-725ac559ae8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 1152, in launch_command\n",
            "    args, defaults, mp_from_config_flag = _validate_launch_command(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 986, in _validate_launch_command\n",
            "    defaults = load_config_from_file(args.config_file)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/config/config_args.py\", line 46, in load_config_from_file\n",
            "    raise FileNotFoundError(\n",
            "FileNotFoundError: The passed configuration file `default_config.yaml` does not exist. Please pass an existing file to `accelerate launch`, or use the default one created through `accelerate config` and run `accelerate launch` without the `--config_file` argument.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!accelerate config"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFSYdqlrfhyO",
        "outputId": "ee5200d0-7ec4-475d-c306-670fe9b98b94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r----------------------------------------------------------------------------------------------------In which compute environment are you running?\n",
            "Please input a choice index (starting from 0), and press enter\n",
            " ➔  \u001b[32mThis machine\u001b[0m\r\n",
            "    AWS (Amazon SageMaker)\n",
            "\u001b[2A\u001b[?25lThis machine\n",
            "\u001b[32mThis machine\u001b[0m\n",
            "----------------------------------------------------------------------------------------------------Which type of machine are you using?\n",
            "Please input a choice index (starting from 0), and press enter\n",
            " ➔  \u001b[32mNo distributed training\u001b[0m\n",
            "    multi-CPU\n",
            "    multi-XPU\n",
            "    multi-GPU\n",
            "    multi-NPU\n",
            "    multi-MLU\n",
            "    multi-MUSA\n",
            "    TPU\n",
            "\u001b[8A\u001b[?25lTPU\n",
            "\u001b[32mNo distributed training\u001b[0m\n",
            "\u001b[?25hDo you want to run your training on CPU only (even if a GPU / Apple Silicon / Ascend NPU device is available)? [yes/NO]:yes\n",
            "Do you want to use Intel PyTorch Extension (IPEX) to speed up training on CPU? [yes/NO]:yes\n",
            "Do you wish to optimize your script with torch dynamo?[yes/NO]:yes\n",
            "----------------------------------------------------------------------------------------------------Which dynamo backend would you like to use?\n",
            "Please input a choice index (starting from 0), and press enter\n",
            "    eager\n",
            "    aot_eager\n",
            " ➔  \u001b[32minductor\u001b[0m\n",
            "    aot_ts_nvfuser\n",
            "    nvprims_nvfuser\n",
            "    cudagraphs\n",
            "    ofi\n",
            "    fx2trt\n",
            "    onnxrt\n",
            "    tensorrt\n",
            "    aot_torchxla_trace_once\n",
            "    torhchxla_trace_once\n",
            "    ipex\n",
            "    tvm\n",
            "\u001b[12A\u001b[?25linductor\n",
            "\u001b[32minductor\u001b[0m\n",
            "\u001b[?25hDo you want to customize the defaults sent to torch.compile? [yes/NO]: yes\n",
            "----------------------------------------------------------------------------------------------------Which mode do you want to use?\n",
            "Please input a choice index (starting from 0), and press enter\n",
            " ➔  \u001b[32mdefault\u001b[0m\n",
            "    reduce-overhead\n",
            "    max-autotune\n",
            "\u001b[3A\u001b[?25ldefault\n",
            "\u001b[32mdefault\u001b[0m\n",
            "\u001b[?25hDo you want the fullgraph mode or it is ok to break model into several subgraphs? [yes/NO]: yes\n",
            "Do you want to enable dynamic shape tracing? [yes/NO]: yes\n",
            "Do you want to use DeepSpeed? [yes/NO]: yes\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/config/config.py\", line 67, in config_command\n",
            "    config = get_user_input()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/config/config.py\", line 40, in get_user_input\n",
            "    config = get_cluster_input()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/config/cluster.py\", line 242, in get_cluster_input\n",
            "    assert (\n",
            "AssertionError: DeepSpeed is not installed => run `pip3 install deepspeed` or build it from source\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!accelerate launch /content/single_gpu.yaml /content/accelerate/examples/inference/distributed/phi2.py"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJII0y14fiRu",
        "outputId": "df270664-b3c7-46d7-979d-9624cd51ae05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install deepspeed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A08FvqyxeLJ5",
        "outputId": "6263d480-c1c0-4bb7-ec88-9791f15eb9bd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepspeed\n",
            "  Downloading deepspeed-0.15.2.tar.gz (1.4 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hjson (from deepspeed)\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.0.8)\n",
            "Collecting ninja (from deepspeed)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.9.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed) (4.66.5)\n",
            "Collecting nvidia-ml-py (from deepspeed)\n",
            "  Downloading nvidia_ml_py-12.560.30-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed) (1.3.0)\n",
            "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_ml_py-12.560.30-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.15.2-py3-none-any.whl size=1521840 sha256=498f39bcf190fc6ab82c4837dda28508e11fa74344939fe8ee969eb4fed1c3f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/f1/46/5d54ff3c9f0af8cab280e54887af31c3a6b489b409ba8e3f5c\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: nvidia-ml-py, ninja, hjson, deepspeed\n",
            "Successfully installed deepspeed-0.15.2 hjson-3.1.0 ninja-1.11.1.1 nvidia-ml-py-12.560.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغال"
      ],
      "metadata": {
        "id": "2DpIlnh5kYvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch --num_processes 1 phi2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCYMqeDOgJSv",
        "outputId": "d7bdf1d1-cf98-4dba-ec20-88503a5a598e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "config.json: 100% 735/735 [00:00<00:00, 3.69MB/s]\n",
            "model.safetensors.index.json: 100% 35.7k/35.7k [00:00<00:00, 83.5MB/s]\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 10.5M/5.00G [00:00<03:35, 23.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 21.0M/5.00G [00:00<03:30, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 31.5M/5.00G [00:01<03:27, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 41.9M/5.00G [00:01<03:25, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 52.4M/5.00G [00:02<03:24, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 62.9M/5.00G [00:02<03:23, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 73.4M/5.00G [00:03<03:23, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 83.9M/5.00G [00:03<03:23, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 94.4M/5.00G [00:03<03:22, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 105M/5.00G [00:04<03:22, 24.2MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 115M/5.00G [00:04<03:22, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 126M/5.00G [00:05<03:21, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 136M/5.00G [00:05<03:21, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 147M/5.00G [00:06<03:21, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 157M/5.00G [00:06<03:20, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 168M/5.00G [00:06<03:20, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 178M/5.00G [00:07<03:20, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 189M/5.00G [00:07<03:21, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 199M/5.00G [00:08<03:19, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 210M/5.00G [00:08<03:18, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 220M/5.00G [00:09<03:17, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 231M/5.00G [00:09<03:17, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 241M/5.00G [00:10<03:16, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 252M/5.00G [00:10<03:22, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 262M/5.00G [00:10<03:14, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 273M/5.00G [00:11<03:14, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 283M/5.00G [00:11<03:14, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 294M/5.00G [00:12<03:13, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 304M/5.00G [00:12<03:13, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 315M/5.00G [00:13<03:13, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 325M/5.00G [00:13<03:12, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 336M/5.00G [00:13<03:13, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 346M/5.00G [00:14<03:12, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 357M/5.00G [00:14<03:12, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 367M/5.00G [00:15<03:11, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 377M/5.00G [00:15<03:10, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 388M/5.00G [00:16<03:10, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 398M/5.00G [00:16<03:10, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 409M/5.00G [00:16<03:15, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 419M/5.00G [00:17<03:12, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 430M/5.00G [00:17<03:17, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 440M/5.00G [00:18<03:14, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 451M/5.00G [00:18<03:13, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 461M/5.00G [00:19<03:11, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 472M/5.00G [00:19<03:09, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 482M/5.00G [00:20<03:09, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 493M/5.00G [00:20<03:09, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 503M/5.00G [00:20<03:06, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 514M/5.00G [00:21<03:06, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 524M/5.00G [00:21<03:07, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 535M/5.00G [00:22<03:05, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 545M/5.00G [00:22<03:04, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 556M/5.00G [00:23<03:04, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 566M/5.00G [00:23<03:03, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 577M/5.00G [00:24<03:04, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 587M/5.00G [00:24<03:03, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 598M/5.00G [00:24<03:02, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 608M/5.00G [00:25<03:02, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 619M/5.00G [00:25<03:01, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 629M/5.00G [00:26<03:00, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 640M/5.00G [00:26<03:00, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 650M/5.00G [00:27<03:00, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 661M/5.00G [00:27<02:59, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 671M/5.00G [00:27<02:58, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 682M/5.00G [00:28<02:58, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 692M/5.00G [00:28<02:57, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 703M/5.00G [00:29<02:57, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 713M/5.00G [00:29<02:57, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 724M/5.00G [00:30<02:57, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 734M/5.00G [00:30<02:57, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 744M/5.00G [00:30<03:00, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 755M/5.00G [00:31<02:54, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 765M/5.00G [00:31<02:54, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 776M/5.00G [00:32<02:54, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 786M/5.00G [00:32<02:54, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 797M/5.00G [00:33<02:53, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 807M/5.00G [00:33<02:53, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 818M/5.00G [00:34<02:53, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 828M/5.00G [00:34<02:52, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 839M/5.00G [00:34<02:51, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 849M/5.00G [00:35<02:51, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 860M/5.00G [00:35<02:52, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 870M/5.00G [00:36<02:51, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 881M/5.00G [00:36<02:50, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 891M/5.00G [00:37<02:49, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 902M/5.00G [00:37<02:55, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 912M/5.00G [00:37<02:52, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 923M/5.00G [00:38<02:50, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 933M/5.00G [00:38<02:49, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 944M/5.00G [00:39<02:48, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 954M/5.00G [00:39<02:47, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 965M/5.00G [00:40<02:46, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 975M/5.00G [00:40<02:46, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 986M/5.00G [00:40<02:45, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 996M/5.00G [00:41<02:47, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.01G/5.00G [00:41<02:43, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.02G/5.00G [00:42<02:43, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.03G/5.00G [00:42<02:43, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.04G/5.00G [00:43<02:43, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.05G/5.00G [00:43<02:43, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.06G/5.00G [00:44<02:43, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.07G/5.00G [00:44<02:43, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.08G/5.00G [00:44<02:43, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.09G/5.00G [00:45<02:42, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.10G/5.00G [00:45<02:41, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.11G/5.00G [00:46<02:41, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.12G/5.00G [00:46<02:40, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.13G/5.00G [00:47<02:46, 23.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.14G/5.00G [00:47<02:38, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.15G/5.00G [00:47<02:39, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.16G/5.00G [00:48<02:39, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.17G/5.00G [00:48<02:38, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.18G/5.00G [00:49<02:38, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.20G/5.00G [00:49<02:37, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.21G/5.00G [00:50<02:36, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.22G/5.00G [00:50<02:36, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.23G/5.00G [00:51<02:35, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.24G/5.00G [00:51<02:35, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.25G/5.00G [00:51<02:34, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.26G/5.00G [00:52<02:34, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.27G/5.00G [00:52<02:34, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.28G/5.00G [00:53<02:34, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.29G/5.00G [00:53<02:34, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.30G/5.00G [00:54<02:36, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.31G/5.00G [00:54<02:34, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.32G/5.00G [00:54<02:33, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.33G/5.00G [00:55<02:33, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.34G/5.00G [00:55<02:31, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.35G/5.00G [00:56<02:37, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.36G/5.00G [00:56<02:40, 22.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.37G/5.00G [00:57<02:39, 22.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.38G/5.00G [00:57<02:36, 23.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.39G/5.00G [00:58<02:35, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.41G/5.00G [00:58<02:31, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.42G/5.00G [00:58<02:30, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.43G/5.00G [00:59<02:29, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.44G/5.00G [00:59<02:28, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.45G/5.00G [01:00<02:26, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.46G/5.00G [01:00<02:26, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.47G/5.00G [01:01<02:28, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.48G/5.00G [01:01<02:26, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.49G/5.00G [01:02<02:27, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.50G/5.00G [01:02<02:45, 21.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.51G/5.00G [01:03<02:38, 21.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.52G/5.00G [01:03<02:39, 21.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.53G/5.00G [01:04<02:36, 22.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.54G/5.00G [01:04<02:31, 22.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.55G/5.00G [01:04<02:28, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.56G/5.00G [01:05<02:25, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.57G/5.00G [01:05<02:31, 22.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.58G/5.00G [01:06<02:28, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.59G/5.00G [01:06<02:29, 22.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.60G/5.00G [01:07<02:26, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.61G/5.00G [01:07<02:25, 23.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.63G/5.00G [01:08<02:23, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.64G/5.00G [01:08<02:21, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.65G/5.00G [01:09<02:25, 23.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.66G/5.00G [01:09<02:22, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.67G/5.00G [01:09<02:20, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.68G/5.00G [01:10<02:19, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.69G/5.00G [01:10<02:19, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.70G/5.00G [01:11<02:18, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.71G/5.00G [01:11<02:17, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.72G/5.00G [01:12<02:16, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.73G/5.00G [01:12<02:15, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.74G/5.00G [01:12<02:15, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.75G/5.00G [01:13<02:15, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.76G/5.00G [01:13<02:15, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.77G/5.00G [01:14<02:17, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.78G/5.00G [01:15<02:52, 18.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.79G/5.00G [01:15<02:42, 19.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.80G/5.00G [01:16<02:55, 18.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.81G/5.00G [01:16<02:56, 18.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.82G/5.00G [01:17<02:48, 18.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.84G/5.00G [01:17<02:50, 18.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.85G/5.00G [01:18<02:43, 19.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.86G/5.00G [01:18<02:45, 19.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.87G/5.00G [01:19<02:39, 19.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.88G/5.00G [01:19<02:33, 20.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.89G/5.00G [01:20<02:38, 19.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.90G/5.00G [01:21<02:52, 18.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.91G/5.00G [01:21<02:59, 17.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.92G/5.00G [01:22<03:03, 16.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.93G/5.00G [01:23<03:40, 13.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.94G/5.00G [01:24<03:50, 13.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.95G/5.00G [01:25<03:56, 12.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.96G/5.00G [01:26<03:58, 12.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.97G/5.00G [01:26<03:51, 13.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.98G/5.00G [01:27<03:52, 13.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.99G/5.00G [01:28<03:53, 12.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 2.00G/5.00G [01:29<03:53, 12.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 2.01G/5.00G [01:30<03:44, 13.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.02G/5.00G [01:31<03:48, 13.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.03G/5.00G [01:31<03:48, 13.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.04G/5.00G [01:32<03:40, 13.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.06G/5.00G [01:33<03:43, 13.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.07G/5.00G [01:34<03:44, 13.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.08G/5.00G [01:34<03:36, 13.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.09G/5.00G [01:35<03:41, 13.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.10G/5.00G [01:36<03:42, 13.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.11G/5.00G [01:37<03:33, 13.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.12G/5.00G [01:38<03:36, 13.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.13G/5.00G [01:38<03:28, 13.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.14G/5.00G [01:39<03:30, 13.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.15G/5.00G [01:40<03:23, 14.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.16G/5.00G [01:40<03:16, 14.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.17G/5.00G [01:41<03:18, 14.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.18G/5.00G [01:42<03:12, 14.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.19G/5.00G [01:43<03:05, 15.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.20G/5.00G [01:43<02:59, 15.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.21G/5.00G [01:44<02:51, 16.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.22G/5.00G [01:44<02:47, 16.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.23G/5.00G [01:45<02:40, 17.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.24G/5.00G [01:45<02:34, 17.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.25G/5.00G [01:46<02:25, 18.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.26G/5.00G [01:46<02:21, 19.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.28G/5.00G [01:47<02:14, 20.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.29G/5.00G [01:47<02:09, 20.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.30G/5.00G [01:48<02:04, 21.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.31G/5.00G [01:48<02:00, 22.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.32G/5.00G [01:49<01:57, 22.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.33G/5.00G [01:49<01:54, 23.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.34G/5.00G [01:50<01:53, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.35G/5.00G [01:50<01:51, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.36G/5.00G [01:51<02:04, 21.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.37G/5.00G [01:51<02:14, 19.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.38G/5.00G [01:52<02:21, 18.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.39G/5.00G [01:52<02:15, 19.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.40G/5.00G [01:53<02:15, 19.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.41G/5.00G [01:53<02:15, 19.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.42G/5.00G [01:54<02:12, 19.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.43G/5.00G [01:55<02:12, 19.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.44G/5.00G [01:55<02:07, 20.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.45G/5.00G [01:56<02:16, 18.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.46G/5.00G [01:56<02:08, 19.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.47G/5.00G [01:57<02:03, 20.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.49G/5.00G [01:57<01:57, 21.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.50G/5.00G [01:57<01:53, 22.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.51G/5.00G [01:58<01:52, 22.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.52G/5.00G [01:58<01:53, 21.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.53G/5.00G [01:59<01:49, 22.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.54G/5.00G [01:59<01:46, 23.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.55G/5.00G [02:00<01:44, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.56G/5.00G [02:00<01:43, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.57G/5.00G [02:01<01:43, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.58G/5.00G [02:01<01:56, 20.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.59G/5.00G [02:02<01:51, 21.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.60G/5.00G [02:02<01:59, 20.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.61G/5.00G [02:03<01:52, 21.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.62G/5.00G [02:03<01:48, 22.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.63G/5.00G [02:04<01:44, 22.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.64G/5.00G [02:04<01:42, 23.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.65G/5.00G [02:04<01:40, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.66G/5.00G [02:05<01:38, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.67G/5.00G [02:05<01:37, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.68G/5.00G [02:06<01:36, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.69G/5.00G [02:06<01:35, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.71G/5.00G [02:07<01:35, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.72G/5.00G [02:07<01:51, 20.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.73G/5.00G [02:08<01:59, 19.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.74G/5.00G [02:08<01:54, 19.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.75G/5.00G [02:09<01:59, 18.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.76G/5.00G [02:10<01:53, 19.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.77G/5.00G [02:10<02:02, 18.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.78G/5.00G [02:11<01:55, 19.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.79G/5.00G [02:11<01:57, 18.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.80G/5.00G [02:12<01:53, 19.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.81G/5.00G [02:12<01:51, 19.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.82G/5.00G [02:13<01:51, 19.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.83G/5.00G [02:13<01:47, 20.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.84G/5.00G [02:14<01:48, 19.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.85G/5.00G [02:14<01:45, 20.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.86G/5.00G [02:15<01:44, 20.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.87G/5.00G [02:15<01:45, 20.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.88G/5.00G [02:16<01:41, 20.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.89G/5.00G [02:16<01:41, 20.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.90G/5.00G [02:17<01:42, 20.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.92G/5.00G [02:17<01:39, 20.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.93G/5.00G [02:18<01:39, 20.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.94G/5.00G [02:18<01:41, 20.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.95G/5.00G [02:19<01:39, 20.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.96G/5.00G [02:20<01:39, 20.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.97G/5.00G [02:20<01:39, 20.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.98G/5.00G [02:21<01:39, 20.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.99G/5.00G [02:21<01:38, 20.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 3.00G/5.00G [02:22<01:36, 20.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 3.01G/5.00G [02:22<01:36, 20.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 3.02G/5.00G [02:23<01:35, 20.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.03G/5.00G [02:23<01:45, 18.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.04G/5.00G [02:24<01:48, 18.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.05G/5.00G [02:25<01:52, 17.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.06G/5.00G [02:25<01:49, 17.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.07G/5.00G [02:26<01:51, 17.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.08G/5.00G [02:26<01:50, 17.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.09G/5.00G [02:27<01:48, 17.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.10G/5.00G [02:27<01:44, 18.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.11G/5.00G [02:28<01:42, 18.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.12G/5.00G [02:29<01:39, 18.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.14G/5.00G [02:29<01:38, 18.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.15G/5.00G [02:30<01:35, 19.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.16G/5.00G [02:30<01:35, 19.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.17G/5.00G [02:31<01:32, 19.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.18G/5.00G [02:31<01:29, 20.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.19G/5.00G [02:32<01:30, 20.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.20G/5.00G [02:32<01:27, 20.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.21G/5.00G [02:33<01:25, 20.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.22G/5.00G [02:33<01:27, 20.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.23G/5.00G [02:34<01:25, 20.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.24G/5.00G [02:34<01:23, 21.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.25G/5.00G [02:35<01:24, 20.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.26G/5.00G [02:35<01:23, 20.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.27G/5.00G [02:36<01:21, 21.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.28G/5.00G [02:36<01:21, 21.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.29G/5.00G [02:37<01:21, 20.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.30G/5.00G [02:37<01:19, 21.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.31G/5.00G [02:38<01:18, 21.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.32G/5.00G [02:38<01:20, 20.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.33G/5.00G [02:39<01:21, 20.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.34G/5.00G [02:39<01:20, 20.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.36G/5.00G [02:40<01:17, 21.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.37G/5.00G [02:40<01:18, 20.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.38G/5.00G [02:41<01:18, 20.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.39G/5.00G [02:41<01:17, 20.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.40G/5.00G [02:42<01:15, 21.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.41G/5.00G [02:42<01:15, 21.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.42G/5.00G [02:43<01:13, 21.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.43G/5.00G [02:43<01:15, 20.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.44G/5.00G [02:44<01:13, 21.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.45G/5.00G [02:44<01:11, 21.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.46G/5.00G [02:45<01:09, 21.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.47G/5.00G [02:45<01:08, 22.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.48G/5.00G [02:46<01:09, 21.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.49G/5.00G [02:46<01:10, 21.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.50G/5.00G [02:47<01:07, 22.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.51G/5.00G [02:47<01:05, 22.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.52G/5.00G [02:47<01:05, 22.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.53G/5.00G [02:48<01:03, 22.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.54G/5.00G [02:48<01:02, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.55G/5.00G [02:49<01:01, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.57G/5.00G [02:49<01:00, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.58G/5.00G [02:50<00:59, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.59G/5.00G [02:50<00:58, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.60G/5.00G [02:50<00:58, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.61G/5.00G [02:51<00:58, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.62G/5.00G [02:51<00:57, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.63G/5.00G [02:52<00:56, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.64G/5.00G [02:52<00:56, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.65G/5.00G [02:53<00:55, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.66G/5.00G [02:53<00:55, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.67G/5.00G [02:54<00:55, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.68G/5.00G [02:54<00:54, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.69G/5.00G [02:54<00:54, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.70G/5.00G [02:55<00:53, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.71G/5.00G [02:55<00:53, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.72G/5.00G [02:56<00:52, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.73G/5.00G [02:56<00:52, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.74G/5.00G [02:57<00:51, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.75G/5.00G [02:57<00:51, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.76G/5.00G [02:58<01:00, 20.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.77G/5.00G [02:58<00:57, 21.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.79G/5.00G [02:59<01:03, 19.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.80G/5.00G [02:59<00:59, 20.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.81G/5.00G [03:00<01:02, 18.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.82G/5.00G [03:00<00:58, 20.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.83G/5.00G [03:01<00:55, 21.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.84G/5.00G [03:01<00:58, 19.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.85G/5.00G [03:02<00:55, 20.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.86G/5.00G [03:02<00:53, 21.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.87G/5.00G [03:03<00:50, 22.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.88G/5.00G [03:03<00:49, 22.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.89G/5.00G [03:04<00:48, 22.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.90G/5.00G [03:04<00:49, 22.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.91G/5.00G [03:05<00:49, 22.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.92G/5.00G [03:05<00:48, 22.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.93G/5.00G [03:06<00:46, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.94G/5.00G [03:06<00:45, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.95G/5.00G [03:06<00:44, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.96G/5.00G [03:07<00:43, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.97G/5.00G [03:07<00:43, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.98G/5.00G [03:08<00:42, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 4.00G/5.00G [03:08<00:42, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 4.01G/5.00G [03:09<00:41, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 4.02G/5.00G [03:09<00:42, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.03G/5.00G [03:10<00:49, 19.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.04G/5.00G [03:11<00:55, 17.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.05G/5.00G [03:11<00:57, 16.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.06G/5.00G [03:12<01:01, 15.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.07G/5.00G [03:13<01:03, 14.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.08G/5.00G [03:14<01:04, 14.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.09G/5.00G [03:14<01:03, 14.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.10G/5.00G [03:15<01:10, 12.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.11G/5.00G [03:16<01:11, 12.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.12G/5.00G [03:17<01:12, 12.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.13G/5.00G [03:18<01:12, 11.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.14G/5.00G [03:19<01:11, 12.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.15G/5.00G [03:20<01:13, 11.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.16G/5.00G [03:22<01:28, 9.46MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.17G/5.00G [03:23<01:42, 8.01MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.18G/5.00G [03:25<01:45, 7.70MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.19G/5.00G [03:26<01:44, 7.65MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.20G/5.00G [03:28<01:53, 6.99MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.22G/5.00G [03:30<01:52, 6.93MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.23G/5.00G [03:31<01:47, 7.14MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.24G/5.00G [03:32<01:44, 7.24MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.25G/5.00G [03:34<01:41, 7.41MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.26G/5.00G [03:35<01:39, 7.39MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.27G/5.00G [03:37<01:40, 7.23MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.28G/5.00G [03:38<01:32, 7.73MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.29G/5.00G [03:39<01:24, 8.32MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.30G/5.00G [03:40<01:17, 9.00MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.31G/5.00G [03:41<01:10, 9.74MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.32G/5.00G [03:41<01:03, 10.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.33G/5.00G [03:42<00:58, 11.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.34G/5.00G [03:43<00:56, 11.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.35G/5.00G [03:44<00:50, 12.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.36G/5.00G [03:44<00:45, 13.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.37G/5.00G [03:45<00:40, 15.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.38G/5.00G [03:45<00:37, 16.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.39G/5.00G [03:46<00:34, 17.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.40G/5.00G [03:46<00:31, 18.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.41G/5.00G [03:47<00:28, 20.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.42G/5.00G [03:47<00:26, 21.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.44G/5.00G [03:48<00:25, 21.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.45G/5.00G [03:48<00:24, 22.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.46G/5.00G [03:48<00:23, 22.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.47G/5.00G [03:49<00:22, 23.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.48G/5.00G [03:49<00:22, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.49G/5.00G [03:50<00:21, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.50G/5.00G [03:50<00:20, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.51G/5.00G [03:51<00:20, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.52G/5.00G [03:51<00:20, 22.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.53G/5.00G [03:52<00:22, 20.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.54G/5.00G [03:52<00:21, 21.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.55G/5.00G [03:53<00:20, 22.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.56G/5.00G [03:53<00:19, 22.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.57G/5.00G [03:54<00:18, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.58G/5.00G [03:54<00:17, 23.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.59G/5.00G [03:54<00:17, 23.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.60G/5.00G [03:55<00:16, 23.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.61G/5.00G [03:55<00:16, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.62G/5.00G [03:56<00:15, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.63G/5.00G [03:56<00:15, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.65G/5.00G [03:57<00:14, 24.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.66G/5.00G [03:57<00:14, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.67G/5.00G [03:57<00:13, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.68G/5.00G [03:58<00:13, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.69G/5.00G [03:58<00:12, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.70G/5.00G [03:59<00:12, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.71G/5.00G [03:59<00:11, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.72G/5.00G [04:00<00:11, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.73G/5.00G [04:00<00:10, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.74G/5.00G [04:01<00:10, 24.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.75G/5.00G [04:01<00:10, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.76G/5.00G [04:02<00:11, 21.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.77G/5.00G [04:02<00:10, 21.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.78G/5.00G [04:03<00:10, 19.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.79G/5.00G [04:03<00:11, 18.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.80G/5.00G [04:04<00:11, 17.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.81G/5.00G [04:05<00:11, 15.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.82G/5.00G [04:06<00:11, 15.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.83G/5.00G [04:06<00:10, 15.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.84G/5.00G [04:07<00:09, 15.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.85G/5.00G [04:08<00:09, 14.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.87G/5.00G [04:09<00:09, 13.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.88G/5.00G [04:09<00:09, 13.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.89G/5.00G [04:10<00:08, 12.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.90G/5.00G [04:11<00:07, 12.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.91G/5.00G [04:12<00:06, 12.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.92G/5.00G [04:13<00:06, 12.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.93G/5.00G [04:14<00:05, 13.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.94G/5.00G [04:14<00:04, 13.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.95G/5.00G [04:15<00:03, 13.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.96G/5.00G [04:16<00:02, 13.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.97G/5.00G [04:17<00:02, 11.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.98G/5.00G [04:18<00:01, 12.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.99G/5.00G [04:19<00:00, 12.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 5.00G/5.00G [04:19<00:00, 19.3MB/s]\n",
            "Downloading shards:  50% 1/2 [04:19<04:19, 259.95s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/564M [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 31.5M/564M [00:00<00:01, 311MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 62.9M/564M [00:00<00:01, 303MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 94.4M/564M [00:00<00:03, 152MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 126M/564M [00:00<00:02, 188MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 157M/564M [00:00<00:02, 195MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 199M/564M [00:00<00:01, 233MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 241M/564M [00:01<00:01, 246MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 273M/564M [00:01<00:01, 255MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 304M/564M [00:01<00:01, 251MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 336M/564M [00:01<00:00, 260MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 367M/564M [00:01<00:00, 246MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 398M/564M [00:01<00:00, 256MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 440M/564M [00:01<00:00, 276MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 472M/564M [00:01<00:00, 257MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 503M/564M [00:02<00:00, 260MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 564M/564M [00:02<00:00, 248MB/s]\n",
            "Downloading shards: 100% 2/2 [04:22<00:00, 131.35s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:24<00:00, 12.08s/it]\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 886kB/s]\n",
            "tokenizer_config.json: 100% 7.34k/7.34k [00:00<00:00, 31.6MB/s]\n",
            "vocab.json: 100% 798k/798k [00:00<00:00, 3.61MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 1.07MB/s]\n",
            "tokenizer.json: 100% 2.11M/2.11M [00:01<00:00, 1.95MB/s]\n",
            "added_tokens.json: 100% 1.08k/1.08k [00:00<00:00, 7.16MB/s]\n",
            "special_tokens_map.json: 100% 99.0/99.0 [00:00<00:00, 672kB/s]\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "['I would like to thank the following people for their help in the preparation of this paper:\\n1. The authors of', 'hello how are you?\") -> \"hello world\"\\n    \"\"\"\\n    return \" \".join(word for word', 'what is going on here?\\n\\nA:\\n\\nYou are using the same variable name for the array and the', 'roses are red and white.\\n    How many red roses are there?\\n    \"\"\"\\n    total_roses', 'welcome to the hotel\"\\n    \"\"\"\\n    return \"welcome to the \" + hotel + \"!\"\\n\\n']\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "# إفراغ ذاكرة وحدة معالجة الرسومات\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# إفراغ ذاكرة الوصول العشوائي\n",
        "gc.collect()\n",
        "\n",
        "# إفراغ ذاكرة القرص (مثال على حذف ملف)\n",
        "!rm -rf /path/to/file"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "w3tcNC7chu03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغال"
      ],
      "metadata": {
        "id": "qZi9xHMjkVXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch --num_processes 1 phi2.py"
      ],
      "metadata": {
        "id": "Rlc-q9sShVIX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce1183b3-31ef-470a-e800-c21728fc9ca8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "Loading checkpoint shards: 100% 2/2 [00:14<00:00,  7.34s/it]\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "['Who is Napoleon Bonaparte?\\n\\nNapoleon Bonaparte was a French military and political leader who rose to prominence during the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XUwAY_m7ks4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GXgkCndIks7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NxMjuf0Cks-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغال"
      ],
      "metadata": {
        "id": "ZuTo4gZamB8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch --num_processes 1 phi2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6rRrHs1ks_w",
        "outputId": "463ae5de-87d4-48a8-a61b-8cb0ce6a9c34"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "config.json: 100% 601/601 [00:00<00:00, 4.19MB/s]\n",
            "model.safetensors.index.json: 100% 23.9k/23.9k [00:00<00:00, 77.9MB/s]\n",
            "Downloading shards:   0% 0/3 [00:00<?, ?it/s]\n",
            "model-00001-of-00003.safetensors:   0% 0.00/4.95G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   1% 31.5M/4.95G [00:00<00:18, 261MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   2% 83.9M/4.95G [00:00<00:13, 354MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   3% 126M/4.95G [00:00<00:13, 364MB/s] \u001b[A\n",
            "model-00001-of-00003.safetensors:   3% 168M/4.95G [00:00<00:13, 344MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   4% 210M/4.95G [00:00<00:14, 321MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   5% 252M/4.95G [00:00<00:14, 329MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   6% 294M/4.95G [00:01<00:34, 134MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   7% 325M/4.95G [00:01<00:32, 144MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   7% 357M/4.95G [00:03<01:51, 41.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   8% 398M/4.95G [00:03<01:16, 59.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   9% 440M/4.95G [00:04<00:55, 80.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  10% 472M/4.95G [00:04<00:45, 97.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  10% 503M/4.95G [00:04<00:37, 118MB/s] \u001b[A\n",
            "model-00001-of-00003.safetensors:  11% 535M/4.95G [00:04<00:31, 142MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  11% 566M/4.95G [00:04<00:26, 166MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  12% 598M/4.95G [00:04<00:23, 182MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  13% 629M/4.95G [00:04<00:22, 192MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  13% 661M/4.95G [00:04<00:21, 202MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  14% 692M/4.95G [00:05<00:20, 203MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  15% 724M/4.95G [00:05<00:20, 209MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  15% 755M/4.95G [00:05<00:19, 219MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  16% 786M/4.95G [00:05<00:19, 218MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  17% 828M/4.95G [00:05<00:16, 248MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  18% 870M/4.95G [00:05<00:15, 270MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  18% 902M/4.95G [00:05<00:15, 255MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  19% 933M/4.95G [00:06<00:16, 251MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  19% 965M/4.95G [00:06<00:15, 249MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  20% 996M/4.95G [00:06<00:15, 252MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  21% 1.03G/4.95G [00:06<00:16, 244MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  21% 1.06G/4.95G [00:06<00:16, 239MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  22% 1.09G/4.95G [00:06<00:15, 257MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  23% 1.12G/4.95G [00:06<00:15, 247MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  23% 1.15G/4.95G [00:06<00:15, 250MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  24% 1.18G/4.95G [00:07<00:15, 241MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  25% 1.22G/4.95G [00:07<00:15, 242MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  25% 1.25G/4.95G [00:07<00:14, 254MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  26% 1.28G/4.95G [00:07<00:14, 245MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  26% 1.31G/4.95G [00:07<00:14, 249MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  27% 1.34G/4.95G [00:07<00:13, 259MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  28% 1.37G/4.95G [00:07<00:14, 243MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  29% 1.43G/4.95G [00:07<00:11, 297MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  30% 1.47G/4.95G [00:08<00:11, 299MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  30% 1.50G/4.95G [00:08<00:11, 301MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  31% 1.53G/4.95G [00:08<00:11, 303MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  32% 1.57G/4.95G [00:08<00:10, 318MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  33% 1.61G/4.95G [00:08<00:10, 326MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  33% 1.66G/4.95G [00:08<00:09, 334MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  34% 1.70G/4.95G [00:08<00:09, 330MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  35% 1.74G/4.95G [00:08<00:10, 304MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  36% 1.77G/4.95G [00:09<00:11, 285MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  37% 1.81G/4.95G [00:09<00:10, 299MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  37% 1.85G/4.95G [00:09<00:10, 288MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  38% 1.89G/4.95G [00:09<00:10, 283MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  39% 1.92G/4.95G [00:09<00:13, 217MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  39% 1.95G/4.95G [00:09<00:15, 191MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  40% 1.98G/4.95G [00:10<00:16, 175MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  40% 2.00G/4.95G [00:10<00:18, 161MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  41% 2.02G/4.95G [00:10<00:18, 161MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  41% 2.04G/4.95G [00:10<00:18, 156MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  42% 2.07G/4.95G [00:10<00:19, 150MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  42% 2.09G/4.95G [00:11<00:24, 116MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  43% 2.11G/4.95G [00:11<00:22, 129MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  43% 2.13G/4.95G [00:11<00:20, 139MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  43% 2.15G/4.95G [00:11<00:25, 112MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  44% 2.17G/4.95G [00:11<00:24, 113MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  44% 2.19G/4.95G [00:12<00:27, 101MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  45% 2.22G/4.95G [00:12<00:21, 127MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  46% 2.25G/4.95G [00:12<00:17, 158MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  46% 2.28G/4.95G [00:12<00:18, 146MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  46% 2.30G/4.95G [00:12<00:19, 137MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  47% 2.32G/4.95G [00:12<00:21, 121MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  47% 2.35G/4.95G [00:13<00:18, 144MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  48% 2.37G/4.95G [00:13<00:23, 110MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  48% 2.39G/4.95G [00:13<00:23, 109MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  49% 2.41G/4.95G [00:13<00:21, 117MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  49% 2.43G/4.95G [00:13<00:19, 127MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  50% 2.46G/4.95G [00:14<00:20, 124MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  50% 2.49G/4.95G [00:16<01:19, 30.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  51% 2.54G/4.95G [00:16<00:42, 56.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  52% 2.58G/4.95G [00:16<00:29, 80.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  53% 2.61G/4.95G [00:16<00:23, 99.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  53% 2.64G/4.95G [00:16<00:18, 122MB/s] \u001b[A\n",
            "model-00001-of-00003.safetensors:  54% 2.67G/4.95G [00:16<00:15, 142MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  55% 2.72G/4.95G [00:16<00:12, 177MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  56% 2.76G/4.95G [00:17<00:10, 209MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  57% 2.80G/4.95G [00:17<00:08, 240MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  57% 2.84G/4.95G [00:17<00:07, 267MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  58% 2.88G/4.95G [00:17<00:07, 287MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  59% 2.93G/4.95G [00:17<00:06, 298MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  60% 2.97G/4.95G [00:17<00:06, 316MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  61% 3.01G/4.95G [00:17<00:06, 297MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  62% 3.05G/4.95G [00:17<00:06, 281MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  62% 3.08G/4.95G [00:18<00:06, 270MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  63% 3.11G/4.95G [00:18<00:07, 260MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  64% 3.15G/4.95G [00:18<00:06, 269MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  64% 3.18G/4.95G [00:18<00:07, 250MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  65% 3.21G/4.95G [00:18<00:07, 244MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  65% 3.24G/4.95G [00:18<00:06, 260MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  66% 3.27G/4.95G [00:18<00:06, 248MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  67% 3.30G/4.95G [00:18<00:06, 259MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  67% 3.33G/4.95G [00:19<00:06, 237MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  68% 3.37G/4.95G [00:19<00:06, 241MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  69% 3.40G/4.95G [00:19<00:06, 245MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  69% 3.43G/4.95G [00:19<00:06, 242MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  70% 3.46G/4.95G [00:19<00:06, 231MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  71% 3.49G/4.95G [00:19<00:06, 217MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  71% 3.52G/4.95G [00:19<00:06, 213MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  72% 3.55G/4.95G [00:20<00:06, 224MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  72% 3.59G/4.95G [00:20<00:05, 238MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  73% 3.62G/4.95G [00:20<00:05, 230MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  74% 3.65G/4.95G [00:20<00:05, 233MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  74% 3.68G/4.95G [00:20<00:05, 236MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  75% 3.71G/4.95G [00:20<00:05, 233MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  76% 3.74G/4.95G [00:20<00:04, 246MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  76% 3.77G/4.95G [00:21<00:04, 241MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  77% 3.81G/4.95G [00:21<00:04, 239MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  78% 3.84G/4.95G [00:21<00:04, 229MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  78% 3.87G/4.95G [00:21<00:04, 225MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  79% 3.90G/4.95G [00:21<00:04, 225MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  79% 3.93G/4.95G [00:21<00:04, 231MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  81% 3.98G/4.95G [00:21<00:03, 268MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  81% 4.03G/4.95G [00:21<00:03, 299MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  82% 4.06G/4.95G [00:22<00:03, 283MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  83% 4.09G/4.95G [00:22<00:02, 287MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  83% 4.13G/4.95G [00:22<00:02, 308MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  84% 4.16G/4.95G [00:22<00:02, 308MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  85% 4.20G/4.95G [00:22<00:02, 332MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  86% 4.25G/4.95G [00:22<00:02, 279MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  86% 4.28G/4.95G [00:22<00:02, 270MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  87% 4.32G/4.95G [00:22<00:02, 275MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  88% 4.35G/4.95G [00:23<00:02, 259MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  89% 4.38G/4.95G [00:23<00:02, 252MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  89% 4.42G/4.95G [00:23<00:01, 271MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  90% 4.46G/4.95G [00:23<00:01, 260MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  91% 4.49G/4.95G [00:23<00:02, 172MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  91% 4.52G/4.95G [00:24<00:02, 182MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  92% 4.55G/4.95G [00:24<00:02, 172MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  93% 4.59G/4.95G [00:24<00:01, 209MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  93% 4.62G/4.95G [00:24<00:01, 222MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  94% 4.66G/4.95G [00:24<00:01, 209MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  95% 4.69G/4.95G [00:24<00:01, 224MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  96% 4.73G/4.95G [00:25<00:01, 201MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  96% 4.76G/4.95G [00:25<00:00, 201MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  97% 4.80G/4.95G [00:25<00:00, 235MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  98% 4.83G/4.95G [00:25<00:00, 183MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  99% 4.88G/4.95G [00:25<00:00, 219MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  99% 4.91G/4.95G [00:25<00:00, 237MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors: 100% 4.95G/4.95G [00:26<00:00, 189MB/s]\n",
            "Downloading shards:  33% 1/3 [00:26<00:53, 26.66s/it]\n",
            "model-00002-of-00003.safetensors:   0% 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   1% 31.5M/5.00G [00:00<00:18, 274MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   1% 62.9M/5.00G [00:00<00:17, 287MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   2% 94.4M/5.00G [00:00<00:16, 298MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   3% 126M/5.00G [00:00<00:16, 303MB/s] \u001b[A\n",
            "model-00002-of-00003.safetensors:   3% 168M/5.00G [00:00<00:15, 314MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   4% 210M/5.00G [00:00<00:14, 330MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   5% 252M/5.00G [00:00<00:14, 336MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   6% 294M/5.00G [00:00<00:14, 315MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   7% 336M/5.00G [00:01<00:14, 318MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   8% 377M/5.00G [00:01<00:15, 299MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   8% 409M/5.00G [00:01<00:15, 296MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   9% 440M/5.00G [00:01<00:15, 289MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   9% 472M/5.00G [00:01<00:18, 249MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  10% 503M/5.00G [00:01<00:17, 250MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  11% 535M/5.00G [00:01<00:17, 258MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  11% 566M/5.00G [00:02<00:21, 205MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  12% 598M/5.00G [00:02<00:23, 191MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  12% 619M/5.00G [00:02<00:23, 184MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  13% 640M/5.00G [00:02<00:28, 156MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  13% 671M/5.00G [00:02<00:23, 187MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  14% 692M/5.00G [00:02<00:23, 184MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  14% 713M/5.00G [00:03<00:26, 161MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  15% 734M/5.00G [00:03<00:28, 152MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  15% 755M/5.00G [00:03<00:28, 148MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  16% 776M/5.00G [00:03<00:27, 153MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  16% 797M/5.00G [00:03<00:28, 148MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  16% 818M/5.00G [00:03<00:27, 154MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  17% 839M/5.00G [00:03<00:26, 157MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  18% 891M/5.00G [00:03<00:17, 232MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  19% 944M/5.00G [00:04<00:14, 285MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  20% 975M/5.00G [00:04<00:13, 289MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  20% 1.01G/5.00G [00:04<00:13, 289MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  21% 1.04G/5.00G [00:04<00:13, 290MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  21% 1.07G/5.00G [00:04<00:13, 284MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  22% 1.11G/5.00G [00:04<00:13, 291MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  23% 1.14G/5.00G [00:04<00:13, 295MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  23% 1.17G/5.00G [00:04<00:12, 298MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  25% 1.23G/5.00G [00:04<00:11, 343MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  25% 1.27G/5.00G [00:05<00:11, 315MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  26% 1.31G/5.00G [00:05<00:13, 267MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  27% 1.34G/5.00G [00:05<00:14, 245MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  27% 1.37G/5.00G [00:05<00:16, 222MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  28% 1.41G/5.00G [00:10<02:25, 24.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  29% 1.43G/5.00G [00:10<01:58, 30.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  29% 1.45G/5.00G [00:10<01:39, 35.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  30% 1.48G/5.00G [00:10<01:10, 50.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  30% 1.50G/5.00G [00:10<00:58, 59.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  30% 1.52G/5.00G [00:10<00:50, 68.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  31% 1.54G/5.00G [00:11<00:49, 70.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  31% 1.56G/5.00G [00:11<00:45, 75.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  32% 1.58G/5.00G [00:11<00:37, 90.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  32% 1.60G/5.00G [00:11<00:35, 96.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  33% 1.65G/5.00G [00:11<00:26, 129MB/s] \u001b[A\n",
            "model-00002-of-00003.safetensors:  33% 1.67G/5.00G [00:11<00:25, 131MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  34% 1.69G/5.00G [00:12<00:24, 134MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  34% 1.71G/5.00G [00:12<00:29, 111MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  35% 1.73G/5.00G [00:12<00:29, 110MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  35% 1.75G/5.00G [00:12<00:27, 120MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  36% 1.79G/5.00G [00:12<00:18, 169MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  36% 1.81G/5.00G [00:12<00:21, 146MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  37% 1.86G/5.00G [00:13<00:16, 193MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  38% 1.90G/5.00G [00:13<00:13, 226MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  39% 1.93G/5.00G [00:13<00:13, 231MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  39% 1.96G/5.00G [00:13<00:13, 226MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  40% 1.99G/5.00G [00:13<00:17, 170MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  40% 2.02G/5.00G [00:13<00:15, 196MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  41% 2.06G/5.00G [00:14<00:15, 194MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  42% 2.10G/5.00G [00:14<00:12, 235MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  43% 2.14G/5.00G [00:14<00:10, 264MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  44% 2.18G/5.00G [00:14<00:09, 292MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  44% 2.22G/5.00G [00:14<00:09, 306MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  45% 2.26G/5.00G [00:14<00:09, 292MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  46% 2.30G/5.00G [00:14<00:09, 296MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  47% 2.34G/5.00G [00:14<00:08, 314MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  48% 2.38G/5.00G [00:15<00:07, 334MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  48% 2.42G/5.00G [00:15<00:09, 282MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  49% 2.45G/5.00G [00:15<00:09, 274MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  50% 2.49G/5.00G [00:15<00:09, 259MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  50% 2.52G/5.00G [00:15<00:09, 251MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  51% 2.55G/5.00G [00:15<00:09, 248MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  52% 2.59G/5.00G [00:15<00:09, 265MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  52% 2.62G/5.00G [00:16<00:10, 236MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  53% 2.65G/5.00G [00:16<00:09, 250MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  54% 2.69G/5.00G [00:16<00:08, 271MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  55% 2.73G/5.00G [00:16<00:09, 246MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  55% 2.77G/5.00G [00:16<00:08, 278MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  56% 2.80G/5.00G [00:16<00:09, 237MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  57% 2.83G/5.00G [00:16<00:09, 239MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  57% 2.86G/5.00G [00:16<00:08, 254MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  58% 2.89G/5.00G [00:17<00:08, 256MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  59% 2.93G/5.00G [00:17<00:07, 260MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  59% 2.96G/5.00G [00:17<00:08, 244MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  60% 2.99G/5.00G [00:17<00:08, 233MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  61% 3.03G/5.00G [00:17<00:07, 264MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  61% 3.06G/5.00G [00:17<00:07, 261MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  62% 3.09G/5.00G [00:17<00:07, 259MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  62% 3.12G/5.00G [00:18<00:07, 236MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  63% 3.16G/5.00G [00:18<00:07, 237MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  64% 3.19G/5.00G [00:18<00:07, 233MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  64% 3.22G/5.00G [00:18<00:07, 249MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  65% 3.26G/5.00G [00:18<00:07, 247MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  66% 3.29G/5.00G [00:18<00:06, 245MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  67% 3.33G/5.00G [00:18<00:06, 269MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  68% 3.38G/5.00G [00:18<00:05, 298MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  68% 3.41G/5.00G [00:19<00:07, 221MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  69% 3.44G/5.00G [00:19<00:07, 206MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  69% 3.47G/5.00G [00:19<00:07, 214MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  70% 3.50G/5.00G [00:19<00:06, 233MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  71% 3.53G/5.00G [00:19<00:06, 233MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  71% 3.57G/5.00G [00:19<00:06, 223MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  72% 3.60G/5.00G [00:20<00:05, 237MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  73% 3.63G/5.00G [00:20<00:05, 245MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  73% 3.66G/5.00G [00:20<00:05, 255MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  74% 3.69G/5.00G [00:20<00:05, 242MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  74% 3.72G/5.00G [00:20<00:05, 247MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  75% 3.75G/5.00G [00:20<00:05, 244MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  76% 3.80G/5.00G [00:20<00:04, 262MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  77% 3.83G/5.00G [00:20<00:04, 240MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  77% 3.86G/5.00G [00:21<00:04, 251MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  78% 3.91G/5.00G [00:21<00:03, 305MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  79% 3.95G/5.00G [00:21<00:03, 323MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  80% 4.00G/5.00G [00:21<00:03, 264MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  81% 4.03G/5.00G [00:21<00:03, 253MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  81% 4.06G/5.00G [00:21<00:03, 262MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  82% 4.10G/5.00G [00:21<00:03, 289MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  83% 4.13G/5.00G [00:22<00:03, 276MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  83% 4.16G/5.00G [00:22<00:02, 281MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  84% 4.19G/5.00G [00:22<00:02, 278MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  85% 4.23G/5.00G [00:24<00:17, 45.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  85% 4.26G/5.00G [00:24<00:12, 59.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  86% 4.29G/5.00G [00:24<00:10, 67.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  86% 4.32G/5.00G [00:24<00:07, 85.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  87% 4.34G/5.00G [00:25<00:07, 85.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  87% 4.36G/5.00G [00:25<00:06, 92.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  88% 4.39G/5.00G [00:25<00:05, 117MB/s] \u001b[A\n",
            "model-00002-of-00003.safetensors:  88% 4.41G/5.00G [00:25<00:05, 108MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  89% 4.46G/5.00G [00:25<00:03, 155MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  90% 4.49G/5.00G [00:26<00:04, 124MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  90% 4.51G/5.00G [00:26<00:04, 115MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  91% 4.53G/5.00G [00:26<00:03, 122MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  91% 4.56G/5.00G [00:26<00:02, 147MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  92% 4.58G/5.00G [00:30<00:20, 20.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  92% 4.62G/5.00G [00:30<00:11, 33.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  93% 4.67G/5.00G [00:30<00:06, 49.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  94% 4.70G/5.00G [00:30<00:04, 64.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  95% 4.73G/5.00G [00:30<00:03, 81.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  95% 4.76G/5.00G [00:31<00:02, 85.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  96% 4.80G/5.00G [00:31<00:01, 116MB/s] \u001b[A\n",
            "model-00002-of-00003.safetensors:  97% 4.84G/5.00G [00:31<00:01, 148MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  98% 4.88G/5.00G [00:31<00:00, 165MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  98% 4.91G/5.00G [00:31<00:00, 177MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  99% 4.94G/5.00G [00:31<00:00, 183MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  99% 4.97G/5.00G [00:36<00:01, 22.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors: 100% 5.00G/5.00G [00:36<00:00, 136MB/s] \n",
            "Downloading shards:  67% 2/3 [01:03<00:32, 32.83s/it]\n",
            "model-00003-of-00003.safetensors:   0% 0.00/4.55G [00:00<?, ?B/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   1% 41.9M/4.55G [00:00<00:13, 336MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   2% 83.9M/4.55G [00:00<00:13, 342MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   3% 126M/4.55G [00:00<00:15, 295MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:   4% 168M/4.55G [00:00<00:13, 316MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   5% 210M/4.55G [00:00<00:14, 307MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   5% 241M/4.55G [00:00<00:13, 308MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   6% 283M/4.55G [00:00<00:14, 302MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   7% 315M/4.55G [00:01<00:13, 305MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   8% 357M/4.55G [00:01<00:13, 310MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   9% 398M/4.55G [00:01<00:12, 321MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  10% 440M/4.55G [00:01<00:15, 273MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  10% 472M/4.55G [00:01<00:15, 258MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  11% 503M/4.55G [00:01<00:17, 236MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  12% 535M/4.55G [00:01<00:16, 240MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  12% 566M/4.55G [00:02<00:15, 255MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  13% 598M/4.55G [00:02<00:14, 267MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  14% 640M/4.55G [00:02<00:12, 302MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  15% 692M/4.55G [00:02<00:11, 340MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  16% 734M/4.55G [00:02<00:17, 223MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  17% 765M/4.55G [00:02<00:16, 226MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  18% 797M/4.55G [00:02<00:15, 242MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  18% 828M/4.55G [00:03<00:14, 256MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  19% 881M/4.55G [00:03<00:12, 291MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  20% 912M/4.55G [00:03<00:15, 231MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  21% 944M/4.55G [00:03<00:15, 233MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  21% 975M/4.55G [00:03<00:14, 239MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  22% 1.01G/4.55G [00:03<00:14, 249MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  23% 1.05G/4.55G [00:03<00:13, 260MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  24% 1.08G/4.55G [00:04<00:16, 216MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  24% 1.11G/4.55G [00:04<00:14, 235MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  25% 1.14G/4.55G [00:04<00:17, 192MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  26% 1.17G/4.55G [00:05<00:33, 101MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  26% 1.20G/4.55G [00:05<00:35, 94.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  27% 1.22G/4.55G [00:05<00:40, 82.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  27% 1.24G/4.55G [00:06<00:43, 75.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  28% 1.28G/4.55G [00:06<00:28, 114MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:  29% 1.31G/4.55G [00:06<00:23, 140MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  30% 1.34G/4.55G [00:06<00:19, 164MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  30% 1.37G/4.55G [00:06<00:17, 179MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  31% 1.41G/4.55G [00:06<00:16, 196MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  32% 1.44G/4.55G [00:06<00:15, 206MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  32% 1.47G/4.55G [00:06<00:14, 216MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  33% 1.51G/4.55G [00:07<00:12, 247MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  34% 1.54G/4.55G [00:07<00:12, 248MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  35% 1.57G/4.55G [00:07<00:12, 235MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  35% 1.60G/4.55G [00:07<00:12, 240MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  36% 1.65G/4.55G [00:07<00:10, 273MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  37% 1.68G/4.55G [00:07<00:10, 279MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  38% 1.71G/4.55G [00:07<00:11, 251MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  38% 1.74G/4.55G [00:08<00:11, 239MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  39% 1.77G/4.55G [00:08<00:11, 235MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  40% 1.80G/4.55G [00:08<00:12, 228MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  40% 1.84G/4.55G [00:08<00:12, 224MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  41% 1.88G/4.55G [00:08<00:10, 257MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  42% 1.91G/4.55G [00:08<00:10, 249MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  43% 1.94G/4.55G [00:08<00:10, 239MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  43% 1.97G/4.55G [00:08<00:10, 246MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  44% 2.00G/4.55G [00:09<00:10, 231MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  45% 2.03G/4.55G [00:09<00:10, 232MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  45% 2.07G/4.55G [00:09<00:11, 224MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  46% 2.10G/4.55G [00:09<00:10, 238MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  47% 2.13G/4.55G [00:09<00:10, 236MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  48% 2.16G/4.55G [00:09<00:09, 241MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  48% 2.19G/4.55G [00:09<00:09, 238MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  49% 2.22G/4.55G [00:10<00:09, 256MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  50% 2.25G/4.55G [00:10<00:08, 263MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  50% 2.29G/4.55G [00:10<00:08, 275MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  51% 2.32G/4.55G [00:10<00:08, 256MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  52% 2.35G/4.55G [00:10<00:08, 251MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  52% 2.38G/4.55G [00:10<00:08, 246MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  53% 2.42G/4.55G [00:10<00:07, 289MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  54% 2.45G/4.55G [00:10<00:07, 290MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  55% 2.50G/4.55G [00:10<00:06, 313MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  56% 2.54G/4.55G [00:11<00:06, 314MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  57% 2.58G/4.55G [00:11<00:07, 273MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  57% 2.61G/4.55G [00:11<00:07, 276MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  58% 2.64G/4.55G [00:11<00:07, 261MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  59% 2.67G/4.55G [00:11<00:07, 236MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  59% 2.71G/4.55G [00:11<00:07, 235MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  60% 2.74G/4.55G [00:11<00:07, 227MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  61% 2.77G/4.55G [00:15<01:09, 25.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  62% 2.82G/4.55G [00:15<00:41, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  63% 2.87G/4.55G [00:16<00:26, 62.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  64% 2.90G/4.55G [00:16<00:23, 70.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  65% 2.94G/4.55G [00:16<00:23, 67.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  65% 2.96G/4.55G [00:17<00:24, 65.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  65% 2.98G/4.55G [00:17<00:25, 61.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  66% 3.00G/4.55G [00:17<00:23, 64.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  67% 3.04G/4.55G [00:18<00:16, 92.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  67% 3.06G/4.55G [00:18<00:14, 105MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:  68% 3.08G/4.55G [00:18<00:12, 116MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  68% 3.10G/4.55G [00:18<00:14, 102MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  69% 3.12G/4.55G [00:18<00:12, 110MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  69% 3.16G/4.55G [00:18<00:09, 141MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  70% 3.19G/4.55G [00:19<00:08, 151MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  71% 3.21G/4.55G [00:19<00:09, 148MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  71% 3.24G/4.55G [00:19<00:07, 181MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  72% 3.27G/4.55G [00:19<00:06, 206MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  73% 3.30G/4.55G [00:19<00:05, 224MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  74% 3.34G/4.55G [00:19<00:04, 268MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  74% 3.38G/4.55G [00:19<00:04, 280MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  75% 3.41G/4.55G [00:19<00:04, 248MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  76% 3.44G/4.55G [00:20<00:04, 223MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  76% 3.47G/4.55G [00:20<00:05, 201MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  77% 3.50G/4.55G [00:20<00:06, 168MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  77% 3.52G/4.55G [00:26<01:07, 15.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  78% 3.54G/4.55G [00:26<00:51, 19.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  78% 3.57G/4.55G [00:26<00:38, 25.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  79% 3.60G/4.55G [00:26<00:25, 37.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  80% 3.64G/4.55G [00:26<00:15, 58.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  81% 3.67G/4.55G [00:26<00:11, 76.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  82% 3.71G/4.55G [00:26<00:07, 108MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:  82% 3.74G/4.55G [00:27<00:06, 130MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  83% 3.77G/4.55G [00:27<00:05, 143MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  84% 3.81G/4.55G [00:27<00:04, 163MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  84% 3.84G/4.55G [00:27<00:03, 182MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  85% 3.87G/4.55G [00:27<00:04, 151MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  86% 3.89G/4.55G [00:33<00:39, 16.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  86% 3.91G/4.55G [00:33<00:30, 20.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  86% 3.93G/4.55G [00:33<00:23, 26.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  87% 3.95G/4.55G [00:33<00:17, 33.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  87% 3.97G/4.55G [00:33<00:13, 41.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  88% 4.00G/4.55G [00:33<00:10, 50.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  88% 4.02G/4.55G [00:34<00:08, 60.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  89% 4.04G/4.55G [00:34<00:06, 73.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  89% 4.07G/4.55G [00:34<00:04, 104MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:  90% 4.10G/4.55G [00:34<00:03, 136MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  91% 4.14G/4.55G [00:34<00:02, 178MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  92% 4.18G/4.55G [00:34<00:01, 215MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  93% 4.22G/4.55G [00:34<00:01, 191MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  93% 4.25G/4.55G [00:35<00:01, 213MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  94% 4.28G/4.55G [00:35<00:01, 216MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  95% 4.31G/4.55G [00:35<00:00, 238MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  95% 4.34G/4.55G [00:35<00:00, 240MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  96% 4.37G/4.55G [00:35<00:00, 246MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  97% 4.40G/4.55G [00:35<00:00, 245MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  98% 4.44G/4.55G [00:35<00:00, 246MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  98% 4.47G/4.55G [00:35<00:00, 257MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  99% 4.50G/4.55G [00:36<00:00, 247MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors: 100% 4.55G/4.55G [00:36<00:00, 125MB/s]\n",
            "Downloading shards: 100% 3/3 [01:40<00:00, 33.52s/it]\n",
            "Loading checkpoint shards: 100% 3/3 [01:04<00:00, 21.43s/it]\n",
            "generation_config.json: 100% 116/116 [00:00<00:00, 846kB/s]\n",
            "tokenizer_config.json: 100% 141k/141k [00:00<00:00, 55.6MB/s]\n",
            "tokenizer.model: 100% 587k/587k [00:00<00:00, 8.22MB/s]\n",
            "tokenizer.json: 100% 1.96M/1.96M [00:01<00:00, 1.83MB/s]\n",
            "special_tokens_map.json: 100% 414/414 [00:00<00:00, 2.29MB/s]\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "['Who is Napoleon Bonaparte?\\n\\nNapoleon Bonaparte was a French military and political leader who rose to prom']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "QwRBJi1uk5mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch --num_processes 1 phi2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgH-KGpAk6AI",
        "outputId": "e57be804-33c3-41cc-b083-1cd840b849d9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "Loading checkpoint shards: 100% 3/3 [01:03<00:00, 21.08s/it]\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "['Who is Napoleon Bonaparte?\\n\\nNapoleon Bonaparte was a French military and political leader who rose to prominence during the French Revolution and its associated wars. He was born on August 15, 1769, on the island of Corsica, which was then a possession of the Republic of Genoa. Napoleon\\'s father, Carlo Buonaparte, was a lawyer and a member of the Corsican nobility, while his mother, Letizia Ramolino, was a woman of humble origins.\\n\\nNapoleon was educated at a Jesuit school in Ajaccio, the capital of Corsica, and later at the Royal Military Academy in Brienne-le-Château, France. He was commissioned as a second lieutenant in the French Army in 1785, and served in various capacities during the French Revolution.\\n\\nIn 1796, Napoleon was given command of the French Army in Italy, where he quickly established a reputation as a brilliant military strategist. He defeated the Austrian forces and expanded French control over much of Italy, earning the nickname \"the Little Corporal.\" In 1799, he staged a coup and took control of the French government, becoming First Consul of the French Republic.\\n\\nAs First Consul, Napoleon consolidated his power and began a series of military campaigns that would make him one of the most powerful men in Europe. He invaded Egypt in 1798, establishing a French presence in the Middle East and challenging British control of the region. He also invaded Switzerland, Austria, and Russia, and defeated the British at the Battle of Trafalgar in 1805.\\n\\nIn 1804, Napoleon crowned himself Emperor of the French, and he ruled as such until his abdication in 1814. During his reign, he implemented a series of political and social reforms, including the creation of a modern legal code, the Code Napoléon, and the promotion of education and economic development.\\n\\nNapoleon was exiled to the island of Elba in 1814, but he escaped and returned to France in 1815. He was defeated at the Battle of Waterloo in June of that year and was exiled to the island of Saint Helena, where he died on May 5, 1821.\\n\\nNapoleon Bonaparte is remembered as one of the most important figures in modern European history. He is credited with transforming France and Europe through his military victories, political reforms, and cultural achievements. He is also known for his ambition, charisma, and intelligence, as well as his ruthless and authoritarian rule.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accelerate/examples/inference/distributed\n",
        "/phi2.py\n",
        "# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "from accelerate import PartialState\n",
        "from accelerate.utils import gather_object\n",
        "\n",
        "\n",
        "# Start up the distributed environment without needing the Accelerator.\n",
        "distributed_state = PartialState()\n",
        "\n",
        "# You can change the model to any LLM such as mistralai/Mistral-7B-v0.1 or meta-llama/Llama-2-7b-chat-hf\n",
        "model_name = \"microsoft/phi-2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, device_map=distributed_state.device, torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Need to set the padding token to the eos token for generation\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "prompts = [\n",
        "    \"I would like to\",\n",
        "    \"hello how are you\",\n",
        "    \"what is going on\",\n",
        "    \"roses are red and\",\n",
        "    \"welcome to the hotel\",\n",
        "]\n",
        "\n",
        "# You can change the batch size depending on your GPU RAM\n",
        "batch_size = 2\n",
        "# We set it to 8 since it is better for some hardware. More information here https://github.com/huggingface/tokenizers/issues/991\n",
        "pad_to_multiple_of = 8\n",
        "\n",
        "# Split into batches\n",
        "# We will get the following results:\n",
        "# [ [\"I would like to\", \"hello how are you\"], [ \"what is going on\", \"roses are red and\"], [ \"welcome to the hotel\"] ]\n",
        "formatted_prompts = [prompts[i : i + batch_size] for i in range(0, len(prompts), batch_size)]\n",
        "\n",
        "# Apply padding on the left since we are doing generation\n",
        "padding_side_default = tokenizer.padding_side\n",
        "tokenizer.padding_side = \"left\"\n",
        "# Tokenize each batch\n",
        "tokenized_prompts = [\n",
        "    tokenizer(formatted_prompt, padding=True, pad_to_multiple_of=pad_to_multiple_of, return_tensors=\"pt\")\n",
        "    for formatted_prompt in formatted_prompts\n",
        "]\n",
        "# Put back the original padding behavior\n",
        "tokenizer.padding_side = padding_side_default\n",
        "\n",
        "completions_per_process = []\n",
        "# We automatically split the batched data we passed to it across all the processes. We also set apply_padding=True\n",
        "# so that the GPUs will have the same number of prompts, and you can then gather the results.\n",
        "# For example, if we have 2 gpus, the distribution will be:\n",
        "# GPU 0: [\"I would like to\", \"hello how are you\"],  \"what is going on\", \"roses are red and\"]\n",
        "# GPU 1: [\"welcome to the hotel\"], [\"welcome to the hotel\"] -> this prompt is duplicated to ensure that all gpus have the same number of prompts\n",
        "with distributed_state.split_between_processes(tokenized_prompts, apply_padding=True) as batched_prompts:\n",
        "    for batch in batched_prompts:\n",
        "        # Move the batch to the device\n",
        "        batch = batch.to(distributed_state.device)\n",
        "        # We generate the text, decode it and add it to the list completions_per_process\n",
        "        outputs = model.generate(**batch, max_new_tokens=20)\n",
        "        generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        completions_per_process.extend(generated_text)\n",
        "\n",
        "# We are gathering string, so we need to use gather_object.\n",
        "# If you need to gather tensors, you can use gather from accelerate.utils\n",
        "completions_gather = gather_object(completions_per_process)\n",
        "\n",
        "# Drop duplicates produced by apply_padding in split_between_processes\n",
        "completions = completions_gather[: len(prompts)]\n",
        "\n",
        "distributed_state.print(completions)"
      ],
      "metadata": {
        "id": "U4fmx2EnnAyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pzqRVZkUnA10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HOzWHv2mnA4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Who is Napoleon Bonaparte?\\n\\nNapoleon Bonaparte was a French military and political leader who rose to prominence during the French Revolution and its associated wars. He was born on August 15, 1769, on the island of Corsica, which was then a possession of the Republic of Genoa. Napoleon\\'s father, Carlo Buonaparte, was a lawyer and a member of the Corsican nobility, while his mother, Letizia Ramolino, was a woman of humble origins.\\n\\nNapoleon was educated at a Jesuit school in Ajaccio, the capital of Corsica, and later at the Royal Military Academy in Brienne-le-Château, France. He was commissioned as a second lieutenant in the French Army in 1785, and served in various capacities during the French Revolution.\\n\\nIn 1796, Napoleon was given command of the French Army in Italy, where he quickly established a reputation as a brilliant military strategist. He defeated the Austrian forces and expanded French control over much of Italy, earning the nickname \"the Little Corporal.\" In 1799, he staged a coup and took control of the French government, becoming First Consul of the French Republic.\\n\\nAs First Consul, Napoleon consolidated his power and began a series of military campaigns that would make him one of the most powerful men in Europe. He invaded Egypt in 1798, establishing a French presence in the Middle East and challenging British control of the region. He also invaded Switzerland, Austria, and Russia, and defeated the British at the Battle of Trafalgar in 1805.\\n\\nIn 1804, Napoleon crowned himself Emperor of the French, and he ruled as such until his abdication in 1814. During his reign, he implemented a series of political and social reforms, including the creation of a modern legal code, the Code Napoléon, and the promotion of education and economic development.\\n\\nNapoleon was exiled to the island of Elba in 1814, but he escaped and returned to France in 1815. He was defeated at the Battle of Waterloo in June of that year and was exiled to the island of Saint Helena, where he died on May 5, 1821.\\n\\nNapoleon Bonaparte is remembered as one of the most important figures in modern European history. He is credited with transforming France and Europe through his military victories, political reforms, and cultural achievements. He is also known for his ambition, charisma, and intelligence, as well as his ruthless and authoritarian rule.']"
      ],
      "metadata": {
        "id": "yIbno0zwmrdh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G5TkJ3vbmFwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D1PlpSw0nCF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o0j0ShmsnCI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ONzvq-XLnCMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/content/a.py\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "prompt = \"Give me a short introduction to large language model.\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
      ],
      "metadata": {
        "id": "6Fb0tmagnCPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch --num_processes 1 a.py"
      ],
      "metadata": {
        "id": "QpcNHFT6nCR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "jchyhC8cq9VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install tensorflow-gpu==2.13.0  # Or your desired TF version\n",
        "!pip install nvidia-tensorrt"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym8fSayWoX8S",
        "outputId": "ddb886b0-1405-4a5d-d05f-76a41520a5f5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-gpu==2.13.0 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.12.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-gpu==2.13.0\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting nvidia-tensorrt\n",
            "  Downloading nvidia_tensorrt-99.0.0-py3-none-manylinux_2_17_x86_64.whl.metadata (596 bytes)\n",
            "Collecting tensorrt (from nvidia-tensorrt)\n",
            "  Downloading tensorrt-10.5.0.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorrt-cu12==10.5.0 (from tensorrt->nvidia-tensorrt)\n",
            "  Downloading tensorrt-cu12-10.5.0.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading nvidia_tensorrt-99.0.0-py3-none-manylinux_2_17_x86_64.whl (17 kB)\n",
            "Building wheels for collected packages: tensorrt, tensorrt-cu12\n",
            "  Building wheel for tensorrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt: filename=tensorrt-10.5.0-py2.py3-none-any.whl size=16338 sha256=63451f63e0c59726f7870b3876c56ad45c462fa344feb628f5182fc81c51ea72\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/93/2c/4db84c8fffd36bab9c54b39b4b886aaaaa1a1ab6d094d28a86\n",
            "  Building wheel for tensorrt-cu12 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt-cu12: filename=tensorrt_cu12-10.5.0-py2.py3-none-any.whl size=17556 sha256=1d0e090d7e4aab72afbcec08710091630960389634e7adfed2b6d51c8b699a2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/83/8a/84edbd0d600c1b334a5ac98e18626a458dc8a70d83d9c5ccbe\n",
            "Successfully built tensorrt tensorrt-cu12\n",
            "Installing collected packages: tensorrt-cu12, tensorrt, nvidia-tensorrt\n",
            "Successfully installed nvidia-tensorrt-99.0.0 tensorrt-10.5.0 tensorrt-cu12-10.5.0\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LIfZXEOoifJ",
        "outputId": "a25a655b-53f0-4db3-9810-15a03881a4bf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-gpu\n",
            "  Using cached tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!nvidia-smi"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "sGtMn9MuojKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "pip install --upgrade pip"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X78M-MBEpOpJ",
        "outputId": "f15bbd63-a0a8-4aa8-bc78-c7de1405ed9d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading pip-24.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-24.2\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import tensorflow as tf\n",
        "   print(tf.__version__)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "4UOzvkoboksw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال"
      ],
      "metadata": {
        "id": "CcdGRFt0yvpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "metadata": {
        "id": "zmVWB9MQnEZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال"
      ],
      "metadata": {
        "id": "0p6tPuY0yzAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/requirements-docs.txt"
      ],
      "metadata": {
        "id": "pWIJ3sCgqTs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال"
      ],
      "metadata": {
        "id": "cDw59OUQytSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install tensorflow[and-cuda]\n",
        "# Verify the installation:\n",
        "!python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\""
      ],
      "metadata": {
        "id": "5sW3h3EiqbzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغال"
      ],
      "metadata": {
        "id": "4NTK7FWLuwFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch --num_processes 1 aa.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umDJEicPrHoV",
        "outputId": "dad96e47-4864-472d-c1e7-7ba983e659cc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "Loading checkpoint shards: 100% 4/4 [01:00<00:00, 15.01s/it]\n",
            "Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "2024-10-14 20:29:02.228082: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-14 20:29:02.509538: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-14 20:29:02.594712: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-14 20:29:03.046002: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-14 20:29:05.239749: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Large Language Models (LLMs) are artificial intelligence systems designed to understand and generate human-like text. They are typically based on transformer architectures and trained on massive amounts of text data from the internet, books, and other sources. These models can perform a wide range of natural language processing tasks, including but not limited to:\n",
            "\n",
            "1. **Text Generation**: Creating coherent paragraphs, stories, or dialogues.\n",
            "2. **Language Translation**: Translating text from one language to another.\n",
            "3. **Question Answering**: Providing answers to questions posed in natural language.\n",
            "4. **Summarization**: Generating concise summaries of longer texts.\n",
            "5. **Text Classification**: Categorizing text into predefined categories.\n",
            "\n",
            "LLMs are often pretrained on very large datasets and then fine-tuned for specific applications. Some well-known examples include models like GPT (Generative Pre-trained Transformer), BERT (Bidirectional Encoder Representations from Transformers), and T5 (Text-to-Text Transfer Transformer).\n",
            "\n",
            "These models have revolutionized the field of natural language processing and are increasingly being used in various industries for tasks such as customer support, content creation, and more.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EZkhXVtwrIb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Who is Napoleon Bonaparte?"
      ],
      "metadata": {
        "id": "iNRwPF_pv7dS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch --num_processes 1 aaa.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0QBHNSGwMMf",
        "outputId": "137ed474-4124-48c2-8156-55906619aeae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "Loading checkpoint shards: 100% 4/4 [01:01<00:00, 15.40s/it]\n",
            "Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "2024-10-14 20:39:51.271209: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-14 20:39:51.548291: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-14 20:39:51.630338: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-14 20:39:52.065972: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-14 20:39:55.357300: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Napoleon Bonaparte (1769-1821) was a French military and political leader who rose to prominence during the French Revolution and its associated wars in Europe. He is widely regarded as one of the greatest commanders of all time and a key figure in shaping the course of modern history.\n",
            "\n",
            "Key points about Napoleon Bonaparte include:\n",
            "\n",
            "1. **Early Life and Military Career**: Born on the island of Corsica, he entered the French military academy at a young age. His exceptional talent for strategy and tactics quickly propelled him to prominence during the French Revolutionary Wars.\n",
            "\n",
            "2. **Coup d'État**: In 1799, he staged a coup d'état and became the First Consul of the French Republic. Four years later, he made himself Emperor of France, thus becoming Napoleon I.\n",
            "\n",
            "3. **Wars and Empire**: Napoleon led numerous successful campaigns against various coalitions of European powers and expanded his empire across much of continental Europe before suffering a series of defeats, notably at the Battle of Waterloo in 1815.\n",
            "\n",
            "4. **Innovations and Reforms**: As emperor, Napoleon implemented several important legal reforms, including the Napoleonic Code, which influenced legal systems around the world. He also standardized weights and measures, promoted education, and supported the arts and sciences.\n",
            "\n",
            "5. **Legacy**: Napoleon's legacy is complex. While he is remembered for his military genius and administrative reforms, his aggressive expansionism and the resulting conflicts with other European powers have also been subjects of extensive historical debate.\n",
            "\n",
            "Napoleon remains an iconic figure in both French and world history, known for his charisma, strategic brilliance, and controversial impact on the political and social landscape of Europe.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/phi2.py======mistralv3\n",
        "/content/aaa.py====Qwen2.5-7B-Instruct\n"
      ],
      "metadata": {
        "id": "ReyGnbW0wN2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "hالاكواد التى اشتغلت جيدا\n",
        "/content/phi2.py\n",
        "/content/aaa.py"
      ],
      "metadata": {
        "id": "sDavnS0sxtVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uGEvoKv3x5sU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phi2.py\n",
        "# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "from accelerate import PartialState\n",
        "from accelerate.utils import gather_object\n",
        "\n",
        "\n",
        "# Start up the distributed environment without needing the Accelerator.\n",
        "distributed_state = PartialState()\n",
        "\n",
        "# You can change the model to any LLM such as mistralai/Mistral-7B-v0.1 or meta-llama/Llama-2-7b-chat-hf\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, device_map=distributed_state.device, torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Need to set the padding token to the eos token for generation\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "prompts = [\n",
        "    \"Who is Napoleon Bonaparte?\",\n",
        "]\n",
        "\n",
        "# You can change the batch size depending on your GPU RAM\n",
        "batch_size = 2\n",
        "# We set it to 8 since it is better for some hardware. More information here https://github.com/huggingface/tokenizers/issues/991\n",
        "pad_to_multiple_of = 8\n",
        "\n",
        "# Split into batches\n",
        "# We will get the following results:\n",
        "# [ [\"I would like to\", \"hello how are you\"], [ \"what is going on\", \"roses are red and\"], [ \"welcome to the hotel\"] ]\n",
        "formatted_prompts = [prompts[i : i + batch_size] for i in range(0, len(prompts), batch_size)]\n",
        "\n",
        "# Apply padding on the left since we are doing generation\n",
        "padding_side_default = tokenizer.padding_side\n",
        "tokenizer.padding_side = \"left\"\n",
        "# Tokenize each batch\n",
        "tokenized_prompts = [\n",
        "    tokenizer(formatted_prompt, padding=True, pad_to_multiple_of=pad_to_multiple_of, return_tensors=\"pt\")\n",
        "    for formatted_prompt in formatted_prompts\n",
        "]\n",
        "# Put back the original padding behavior\n",
        "tokenizer.padding_side = padding_side_default\n",
        "\n",
        "completions_per_process = []\n",
        "# We automatically split the batched data we passed to it across all the processes. We also set apply_padding=True\n",
        "# so that the GPUs will have the same number of prompts, and you can then gather the results.\n",
        "# For example, if we have 2 gpus, the distribution will be:\n",
        "# GPU 0: [\"I would like to\", \"hello how are you\"],  \"what is going on\", \"roses are red and\"]\n",
        "# GPU 1: [\"welcome to the hotel\"], [\"welcome to the hotel\"] -> this prompt is duplicated to ensure that all gpus have the same number of prompts\n",
        "with distributed_state.split_between_processes(tokenized_prompts, apply_padding=True) as batched_prompts:\n",
        "    for batch in batched_prompts:\n",
        "        # Move the batch to the device\n",
        "        batch = batch.to(distributed_state.device)\n",
        "        # We generate the text, decode it and add it to the list completions_per_process\n",
        "        outputs = model.generate(**batch, max_new_tokens=1024)\n",
        "        generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        completions_per_process.extend(generated_text)\n",
        "\n",
        "# We are gathering string, so we need to use gather_object.\n",
        "# If you need to gather tensors, you can use gather from accelerate.utils\n",
        "completions_gather = gather_object(completions_per_process)\n",
        "\n",
        "# Drop duplicates produced by apply_padding in split_between_processes\n",
        "completions = completions_gather[: len(prompts)]\n",
        "\n",
        "distributed_state.print(completions)\n"
      ],
      "metadata": {
        "id": "OvO-Cwb0x-yY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/content/aaa.py\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",          # Automatically choose float32 or float16 based on device\n",
        "    device_map=\"auto\"            # Automatically map layers to available devices (e.g., GPU)\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define prompt and messages\n",
        "prompt = \"Who is Napoleon Bonaparte?\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "\n",
        "# Prepare inputs for the model\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate output from the model\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "\n",
        "# Post-process generated output\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "# Print the response from the model\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "sPvdkOvEyMgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/content/aa.py\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",          # Automatically choose float32 or float16 based on device\n",
        "    device_map=\"auto\"            # Automatically map layers to available devices (e.g., GPU)\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define prompt and messages\n",
        "prompt = \"Give me a short introduction to large language models.\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "\n",
        "# Prepare inputs for the model\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate output from the model\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "\n",
        "# Post-process generated output\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "# Print the response from the model\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "7ywtrZkcySF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/content/requirements-docs.txt\n",
        "furo\n",
        "myst-parser==4.0.0\n",
        "sphinx<8\n",
        "sphinx-copybutton\n",
        "sphinx-design>=0.6.0"
      ],
      "metadata": {
        "id": "NvSWx1ZIyYbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "prompt = \"Give me a short introduction to large language model.\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
      ],
      "metadata": {
        "id": "scvVtl-Yyd45"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}